{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267af5dd-e83a-4431-8036-c75321cb3945",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **ExpLogic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568cabd-3e11-474b-af13-f6c0c668454d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Setting up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65bb0b-594d-4717-bd3c-63b04b9d864b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5dd1c0-73cf-4679-ae21-6fa00a85a395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import yaml\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Compose, Lambda\n",
    "import mnist_dataset\n",
    "from hydra import initialize, compose\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n",
    "from difflogic import LogicLayer, GroupSum, PackBitsTensor\n",
    "from mnist_dataset import MNISTRemoveBorderTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99c41f-b5c7-4d64-a704-5a003a5b56c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8096d-fc17-46d7-861e-685d2fe12dbb",
   "metadata": {},
   "source": [
    "Tunable Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ea8409-a084-4bee-8121-96acd99bb28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurable options\n",
    "remove_border = True       # True: Removes border of Mnist, False: Keeps black border around digits\n",
    "binarize_images = True     # True: Binarized Images, False: Grayscale Images \n",
    "evenly_partitioned = True  # True: Even distribution of samples, False: Original Mnist distribution\n",
    "upscaled_images = False    # True: Upscales the samples to 32x32, False: Keeps size unchanged\n",
    "downscaled_images = False   # True: Downscales the samples to 16x16, False: Keeps size unchanged\n",
    "batch_size = 256           # Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63667e4-7675-4c59-a6f3-18f4acd21475",
   "metadata": {},
   "source": [
    "Set Seed for Reproducibiity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe299cc0-4872-4902-94f7-b85c193bd43f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)      # PyTorch seed fixing\n",
    "torch.cuda.manual_seed(42) # PyTorch CUDA seed fixing (if using GPU)\n",
    "np.random.seed(42)         # NumPy seed fixing\n",
    "random.seed(42)            # Python's built-in random seed fixing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4450630-e7a8-4564-b106-6e2dc3a12e61",
   "metadata": {},
   "source": [
    "Dataset Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1779afcd-2def-41d1-abdc-c1bc8dbb1719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to binarize an image, threshold is tunable \n",
    "def binarize(image, threshold=0.5):\n",
    "    return (image > threshold).float()  \n",
    "\n",
    "# define the transformation logic based on the toggle\n",
    "transform_list = [ToTensor()]\n",
    "\n",
    "# removes border around each mnist digit\n",
    "if remove_border:\n",
    "    transform_list.append(MNISTRemoveBorderTransform())\n",
    "\n",
    "# upscales or downscales the images\n",
    "if upscaled_images:\n",
    "    transform_list.append(Resize((32, 32)))\n",
    "elif downscaled_images:\n",
    "    transform_list.append(Resize((16, 16)))\n",
    "\n",
    "# binarizes the images\n",
    "if binarize_images:\n",
    "    transform_list.append(Lambda(lambda x: binarize(x)))\n",
    "    \n",
    "transform = Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d9255d2-bf0c-4bab-b66a-b65360bcb86f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = mnist_dataset.MNIST('./data-mnist', train=True, download=True, transform=transform)\n",
    "test_dataset = mnist_dataset.MNIST('./data-mnist', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a5db70-d2ba-44a7-803c-829177a182c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes the Dataset evenly partitioned\n",
    "if evenly_partitioned:\n",
    "    # code below is used so that all classes have the same number of samples\n",
    "    train_targets = train_loader.dataset.targets\n",
    "    test_targets = test_loader.dataset.targets\n",
    "\n",
    "    train_digits_total = []\n",
    "    test_digits_total = []\n",
    "\n",
    "    for i in range(10):\n",
    "        curr_tot_train = torch.sum(train_targets == i).item()\n",
    "        curr_tot_test = torch.sum(test_targets == i).item()    \n",
    "        train_digits_total.append(curr_tot_train)\n",
    "        test_digits_total.append(curr_tot_test)\n",
    "\n",
    "    train_digits_total, test_digits_total\n",
    "\n",
    "    # find the minimum number of samples across all classes\n",
    "    min_samples_train = min(train_digits_total)\n",
    "    min_samples_test = min(test_digits_total)\n",
    "\n",
    "    # function to trim dataset to match the minimum samples for each class and shuffle indices\n",
    "    def trim_dataset(dataset, targets, min_samples):\n",
    "        indices = []\n",
    "        for i in range(10):\n",
    "            class_indices = (targets == i).nonzero(as_tuple=True)[0]  # get indices of class i\n",
    "            class_indices = class_indices[:min_samples]  # trim to min_samples\n",
    "            indices.extend(class_indices)\n",
    "\n",
    "        # shuffle indices after collecting them\n",
    "        indices = torch.tensor(indices)\n",
    "        indices = indices[torch.randperm(indices.size(0))]  \n",
    "\n",
    "        return Subset(dataset, indices)\n",
    "\n",
    "    # trim both train and test datasets to ensure all classes have the same number of samples\n",
    "    trimmed_train_dataset = trim_dataset(train_loader.dataset, train_targets, min_samples_train)\n",
    "    trimmed_test_dataset = trim_dataset(test_loader.dataset, test_targets, min_samples_test)\n",
    "\n",
    "    # create DataLoaders for the trimmed datasets\n",
    "    trimmed_train_loader = DataLoader(trimmed_train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    trimmed_test_loader = DataLoader(trimmed_test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # verify the lengths of the trimmed datasets\n",
    "    len(trimmed_train_loader.dataset), len(trimmed_test_loader.dataset)\n",
    "\n",
    "    train_dataset = trimmed_train_dataset\n",
    "    test_dataset = trimmed_test_dataset\n",
    "    train_loader = trimmed_train_loader\n",
    "    test_loader = trimmed_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66230413-408e-4bc1-bf5f-2313690e0964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14e633e1eb80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing a single image\n",
    "dataset_size = len(train_dataset)\n",
    "random_index = random.randint(0, dataset_size - 1)\n",
    "\n",
    "if remove_border and not downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(20, 20)\n",
    "elif not remove_border and not upscaled_images and not downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(28, 28)\n",
    "elif downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(16, 16)\n",
    "else:\n",
    "    image = train_loader.dataset[random_index][0].reshape(32, 32)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973f0e3-5f32-4793-afc6-9394425419de",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e63be-c489-4aaf-a605-9e1d0fda631f",
   "metadata": {},
   "source": [
    "Converts csv into yaml config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b89fd2-f200-40c1-973a-489ef4190e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define first input and the name of the file to be saved\n",
    "first_in_dim = 400 # 20x20\n",
    "filename = \"config/mnist_config_20x20.yaml\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc64546-f1d7-4e18-b88c-7140d4d42692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file 'config/mnist_config_20x20.yaml' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# reads the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"config/mnist_hyperparameters.csv\")\n",
    "\n",
    "# convert the DataFrame to a list of dictionaries\n",
    "models = df.to_dict(orient=\"records\")\n",
    "\n",
    "# create the YAML structure\n",
    "yaml_structure = {\"models\": {}}\n",
    "\n",
    "# rounds the number to the nearest multiple of the output size\n",
    "def round_to_nearest_multiple(value, multiple):\n",
    "    return multiple * round(value / multiple)\n",
    "\n",
    "# populate the YAML structure with models\n",
    "for i, model in enumerate(models, start=1):\n",
    "    # zero-padding model names to 3 digits \n",
    "    model_name = f\"model_{str(i).zfill(3)}\"\n",
    "    layers_config = {}\n",
    "    \n",
    "    for layer in range(1, model[\"H\"] + 1):\n",
    "        # zero-padding the layer names to 3 digits\n",
    "        layer_name = f\"LogicLayer{str(layer).zfill(3)}\"\n",
    "        \n",
    "        # adjusts in_dim to the nearest multiple of 10\n",
    "        in_dim = first_in_dim if layer == 1 else round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        # adjusts out_dim to the nearest multiple of 10\n",
    "        out_dim = round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        layers_config[layer_name] = {\n",
    "            \"in_dim\": in_dim,\n",
    "            \"out_dim\": out_dim,\n",
    "            \"device\": \"cuda\",\n",
    "            \"implementation\": \"cuda\",\n",
    "            \"connections\": \"random\",\n",
    "            \"grad_factor\": 2, # we can try different grad_factor values as well\n",
    "        }\n",
    "    \n",
    "    yaml_structure[\"models\"][model_name] = {\n",
    "        \"input_dim\": first_in_dim, \n",
    "        \"output_size\": 10, # for MNIST classification\n",
    "        \"tau\": model[\"tau\"],\n",
    "        \"learning_rate\": model[\"lr\"],\n",
    "        \"layers_config\": layers_config,\n",
    "    }\n",
    "\n",
    "# saves to a YAML file\n",
    "with open(f'{filename}', \"w\") as file:\n",
    "    yaml.dump(yaml_structure, file, default_flow_style=False)\n",
    "\n",
    "print(f\"YAML file '{filename}' generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e939ace-e4c3-4781-bdea-f9722ddb925e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Model Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4c832-0caf-4efc-9b71-8c2cf1faffa7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Model Function Declarations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb42a7a7-1a8b-4d5f-b2c6-6f68678e328b",
   "metadata": {},
   "source": [
    "Custom GroupSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1ead0c5-4377-4447-937c-353d12632fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomGroupSum(GroupSum):\n",
    "    \"\"\"\n",
    "    The CustomGroupSum module that extends GroupSum to include printing of class sums.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        # Use the original functionality from GroupSum\n",
    "        output = super().forward(x)\n",
    "\n",
    "        # Calculate the class sums for printing\n",
    "        if isinstance(x, PackBitsTensor):\n",
    "            class_sums = x.group_sum(self.k)\n",
    "        else:\n",
    "            class_sums = x.reshape(*x.shape[:-1], self.k, x.shape[-1] // self.k).sum(-1)\n",
    "\n",
    "        # Print the class sums before returning the output\n",
    "        print(\"Class Sums:\", class_sums)\n",
    "\n",
    "        # Return the processed output, as in the original GroupSum\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473370c-ec74-455b-854c-f597d04144f3",
   "metadata": {},
   "source": [
    "DiffLogic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b244edec-9bd5-474d-9aaa-4f9aafe31b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffLogic(nn.Module):\n",
    "    def __init__(self, layers_config, output_size, tau=30):\n",
    "        \"\"\"\n",
    "        Initializes the DiffLogic model with the specified layer configurations, output size, and temperature parameter.\n",
    "\n",
    "        Args:\n",
    "            layers_config (dict): Configuration for each logic layer, including dimensions, device, implementation, connections, and grad factor.\n",
    "            output_size (int): The number of output groups (classes in a classification problem).\n",
    "            tau (int): Temperature parameter for the GroupSum operation.\n",
    "        \"\"\"\n",
    "        super(DiffLogic, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # stores the logic layers\n",
    "        layers = []\n",
    "        for layer_name, config in layers_config.items():\n",
    "            layer = LogicLayer(\n",
    "                in_dim=config['in_dim'],\n",
    "                out_dim=config['out_dim'],\n",
    "                device=config['device'],\n",
    "                implementation=config['implementation'],\n",
    "                connections=config['connections'],\n",
    "                grad_factor=config['grad_factor']       \n",
    "            )\n",
    "            layers.append(layer)\n",
    "            print(layer)\n",
    "        \n",
    "        self.logic_layers = nn.Sequential(*layers)\n",
    "        self.group = GroupSum(k=output_size, tau=tau)\n",
    "        #self.group = CustomGroupSum(k=output_size, tau=tau) \n",
    "        self.log_text = \"\"  # initializes logging string\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DiffLogic model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after processing through the logic layers and grouping operation.\n",
    "        \"\"\"\n",
    "        # moves tensor to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to('cuda')          \n",
    "        x = self.flatten(x)\n",
    "        logits = self.logic_layers(x)\n",
    "        group = self.group(logits)\n",
    "        return group\n",
    "    \n",
    "    def save(self, file_path, model_name='model'):\n",
    "        \"\"\"\n",
    "        Saves the model's state dictionary to the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path where the model will be saved.\n",
    "            model_name (str): Name of the saved model\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'connections': [layer.indices for layer in self.logic_layers if isinstance(layer, LogicLayer)]\n",
    "        }, os.path.join(file_path, f\"{model_name}.pth\"))\n",
    "        self.log_text += f\"Model saved to: {file_path}\\n\"\n",
    "\n",
    "    def load(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads the model's state dictionary from the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path from which the model will be loaded.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # assigns connections to each LogicLayer\n",
    "        for idx, layer in enumerate(self.logic_layers):\n",
    "            if isinstance(layer, LogicLayer):\n",
    "                layer.indices = checkpoint['connections'][idx]\n",
    "\n",
    "        self.eval()\n",
    "        self.log_text += f\"Model loaded from: {file_path}\\n\"\n",
    "        \n",
    "    def get_accuracy(self, data_loader):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy of the model against a data loader\n",
    "\n",
    "        Args:\n",
    "            data_loader: a DataLoader object, e.g. train_loader or test_loader\n",
    "\n",
    "        Returns:\n",
    "            float: The accuracy\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # ensures that model is in evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            for batch_inputs, batch_outputs in tqdm(data_loader, desc=\"Running Inference\"):\n",
    "                batch_inputs, batch_outputs = batch_inputs.to('cuda'), batch_outputs.to('cuda')\n",
    "\n",
    "                # forward pass to get predictions\n",
    "                outputs = self(batch_inputs)\n",
    "\n",
    "                # gets the predicted class (index of the maximum logit)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # counting correct predictions\n",
    "                total += batch_outputs.size(0)  # total number of samples in the batch\n",
    "                correct += (predicted == batch_outputs).sum().item()  # counting correct predictions\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def get_log(self):\n",
    "        \"\"\"\n",
    "        Retrieves the log text and clears the log after retrieval.\n",
    "\n",
    "        Returns:\n",
    "            str: The log text.\n",
    "        \"\"\"\n",
    "        log_copy = self.log_text\n",
    "        self.log_text = \"\"  # Clear the log after returning\n",
    "        return log_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5632fa27-5e05-41b0-b3a3-ac941714657b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        \"\"\"\n",
    "        Initializes the EarlyStopper to stop training if the performance doesn't improve after a certain number of epochs.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait for an improvement.\n",
    "            min_delta (float): Minimum change to consider an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def should_stop(self, current_loss):\n",
    "        \"\"\"\n",
    "        Check if training should stop based on the current loss.\n",
    "\n",
    "        Args:\n",
    "            current_loss (float): The current loss.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if training should stop, False otherwise.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = current_loss\n",
    "            return False\n",
    "        elif current_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(\"EarlyStopper Triggered: \", self.counter)\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd97e0b-c6c4-4262-97ab-32136126ba39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea90015-6e45-49af-bb11-539c8811adcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize Hydra with the config path and job name\n",
    "with initialize(version_base=None, config_path=\"config\", job_name=\"ExpLogic\"):\n",
    "    cfg = compose(config_name=\"mnist_config_20x20\")\n",
    "\n",
    "# training loop for all models\n",
    "all_models_dict = {}\n",
    "num_epochs = 5\n",
    "file_path = 'trained_models/mnist_trained_20x20' # where to save your trained models\n",
    "\n",
    "# loops through all model configs and trains each of them\n",
    "for model_name, model_cfg in cfg.models.items():\n",
    "    print(f'training model {model_name}')\n",
    "\n",
    "    # tracking dictionary\n",
    "    all_models_dict[model_name] = {\n",
    "        'losses': [],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # initializes DiffLogic model and moves to CUDA if available\n",
    "        model = DiffLogic(layers_config=model_cfg['layers_config'], \n",
    "                          output_size=model_cfg['output_size'], \n",
    "                          tau=model_cfg['tau']).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # optimizer and loss criterion\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=model_cfg['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # early stopping\n",
    "        early_stopper = EarlyStopper(patience=5)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "            epoch_loss = 0  # to track loss for an epoch\n",
    "            \n",
    "            for batch_inputs, batch_outputs in loop:\n",
    "                # move data to the appropriate device\n",
    "                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "                batch_inputs, batch_outputs = batch_inputs.to(device).double(), batch_outputs.to(device).long()\n",
    "\n",
    "                # forward pass through the model\n",
    "                predictions = model(batch_inputs)\n",
    "                loss = criterion(predictions, batch_outputs)\n",
    "\n",
    "                # zero gradients, backpropagates, and updates model parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # accumulating the loss for the epoch\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # caclulating the average loss for the epoch\n",
    "            epoch_loss /= len(train_loader)\n",
    "            all_models_dict[model_name]['losses'].append(epoch_loss)\n",
    "            print(f'Epoch {epoch+1} Loss: {epoch_loss}')\n",
    "\n",
    "            # checks for early stopping\n",
    "            if early_stopper.should_stop(epoch_loss):\n",
    "                print(f\"Early stopping triggered for {model_name} at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "        # saving trained model's state\n",
    "        model.save(file_path, model_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR TRAINING {model_name.upper()}: {str(e)}\")\n",
    "\n",
    "print(\"All models processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28a834-adac-4241-83a4-d9a23ac17db5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Model Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33048f62-fc21-4f87-b725-6eb470a66700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicLayer(400, 2500, train)\n",
      "LogicLayer(2500, 2500, train)\n",
      "Evaluating model_077_weights.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:03<00:00, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_077_weights.pth: 92.15%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testing loop to test inferences\n",
    "trained_models_dir = 'trained_models/'\n",
    "\n",
    "# retrieves a list of all model files in the directory\n",
    "model_files = sorted([f for f in os.listdir(trained_models_dir) if f.endswith('.pth')])\n",
    "\n",
    "with initialize(version_base=None, config_path=\"config\", job_name=\"ExpLogic_Test\"):\n",
    "    cfg = compose(config_name=\"mnist_config_20x20\")\n",
    "\n",
    "# dictionary to store the trained models\n",
    "trained_models = {}\n",
    "trained_models_accuracies = {}\n",
    "\n",
    "# loops through all model files and calculates their accuracies\n",
    "for i, model_file in enumerate(model_files):\n",
    "    if model_file.endswith('_weights.pth'):\n",
    "        model_name = model_file.removesuffix('_weights.pth')\n",
    "    else:\n",
    "        model_name = model_file.removesuffix('.pth')\n",
    "    \n",
    "    model_cfg = cfg['models'][model_name]\n",
    "    \n",
    "    # instantiates the model and load its weights\n",
    "    model = DiffLogic(layers_config=model_cfg['layers_config'], \n",
    "                          output_size=model_cfg['output_size'], \n",
    "                          tau=model_cfg['tau']).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_path = os.path.join(trained_models_dir, model_file)\n",
    "    print(f\"Evaluating {model_file}...\")\n",
    "\n",
    "    # loads the respective model\n",
    "    model.load(model_path)\n",
    "    \n",
    "    # calculates accuracy\n",
    "    accuracy = model.get_accuracy(test_loader)\n",
    "    \n",
    "    print(f\"Accuracy of {model_file}: {accuracy * 100:.2f}%\\n\")\n",
    "    \n",
    "    trained_models[i] = model\n",
    "    trained_models_accuracies[i] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc15d31-2d9b-42f4-8d07-039e9c972871",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Switching Probability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decdce79-f993-4a5c-81d1-f0e5b64be619",
   "metadata": {},
   "source": [
    "Converting DiffLogic to LogicGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f72fa04-489d-4d62-98c0-fd2b9323c677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LogicGraph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MNIST images: 100%|██████████| 211/211 [00:20<00:00, 10.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LogicGraph import * \n",
    "    \n",
    "print(\"Creating LogicGraph\")\n",
    "gall = LogicGraph(model)\n",
    "\n",
    "gall.compute_sf(train_loader)\n",
    "gall.compute_sp(np.ones((20,20))-0.5)\n",
    "\n",
    "gall.edges()[('L1_N2029', 'L2_N2124')][\"ab\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70e7db-e9f7-400d-998f-79e7773da32d",
   "metadata": {},
   "source": [
    "Difference of SP for Individual Classes\n",
    "\n",
    "Find the TP, TN, FP, FN instances, and use this to create sub-datasets for specific cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81293fce-d505-43d5-aa21-0011f72df319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/all_images_train.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(all_predictions_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./datasets/all_predictions_test.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m---> 51\u001b[0m     all_images_train      \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./datasets/all_images_train.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     all_labels_train      \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./datasets/all_labels_train.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m     all_predictions_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./datasets/all_predictions_train.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/blue/woodard/mkunzlermaldaner/project/ExpLogic/explogic_env/lib/python3.9/site-packages/torch/serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    592\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/blue/woodard/mkunzlermaldaner/project/ExpLogic/explogic_env/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/blue/woodard/mkunzlermaldaner/project/ExpLogic/explogic_env/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/all_images_train.pth'"
     ]
    }
   ],
   "source": [
    "saved = True \n",
    "\n",
    "def predict_and_categorize(model, data_loader, binarize=False, threshold=0.5):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.diff_logic_model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for batch_inputs, batch_outputs in tqdm(data_loader, desc=\"Predicting\"):\n",
    "            batch_inputs, batch_outputs = batch_inputs.to('cuda'), batch_outputs.to('cuda')\n",
    "\n",
    "            if binarize:\n",
    "                batch_inputs = (batch_inputs > threshold).float()\n",
    "\n",
    "            # Forward pass to get predictions\n",
    "            outputs = model.diff_logic_model(batch_inputs)\n",
    "\n",
    "            # Get the predicted class (index of the maximum logit)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_images.append(batch_inputs.cpu())\n",
    "            all_labels.append(batch_outputs.cpu())\n",
    "            all_predictions.append(predicted.cpu())\n",
    "\n",
    "    all_images = torch.cat(all_images)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "    return all_images, all_labels, all_predictions\n",
    "\n",
    "if not saved: \n",
    "    # Usage:\n",
    "    binarize = True  # Set to True if you want to binarize the images\n",
    "    threshold = 0.5  # Set the threshold for binarization if binarize is True\n",
    "\n",
    "    all_images_train, all_labels_train, all_predictions_train = predict_and_categorize(model, train_loader, binarize=binarize, threshold=threshold)\n",
    "    all_images_test,  all_labels_test,  all_predictions_test  = predict_and_categorize(model,  test_loader, binarize=binarize, threshold=threshold) \n",
    "\n",
    "    torch.save(     all_images_train, './datasets/all_images_train.pth')\n",
    "    torch.save(     all_labels_train, './datasets/all_labels_train.pth')\n",
    "    torch.save(all_predictions_train, './datasets/all_predictions_train.pth')\n",
    "\n",
    "    torch.save(     all_images_test, './datasets/all_images_test.pth')\n",
    "    torch.save(     all_labels_test, './datasets/all_labels_test.pth')\n",
    "    torch.save(all_predictions_test, './datasets/all_predictions_test.pth')\n",
    "\n",
    "else: \n",
    "    all_images_train      = torch.load('./datasets/all_images_train.pth')\n",
    "    all_labels_train      = torch.load('./datasets/all_labels_train.pth')\n",
    "    all_predictions_train = torch.load('./datasets/all_predictions_train.pth')\n",
    "\n",
    "    all_images_test       = torch.load('./datasets/all_images_test.pth')\n",
    "    all_labels_test       = torch.load('./datasets/all_labels_test.pth')\n",
    "    all_predictions_test  = torch.load('./datasets/all_predictions_test.pth')\n",
    "\n",
    "if not saved: \n",
    "    # Fix random seeds for reproducibility\n",
    "    torch.manual_seed(42)            \n",
    "    torch.cuda.manual_seed(42)        \n",
    "    np.random.seed(42)\n",
    "    # If using CUDA:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Init parameters\n",
    "    batch_size = 256   # this can be tuned as well \n",
    "    binarize   = True  # Set this to True if you want to binarize the images\n",
    "    threshold  = 0.5   # Set the threshold for binarization\n",
    "\n",
    "    # Create class-specific data loaders for training and testing sets\n",
    "    train_class_loaders = subset.create_class_dataloaders(train_dataset, batch_size, binarize=binarize, threshold=threshold)\n",
    "    test_class_loaders  = subset.create_class_dataloaders(test_dataset,  batch_size, binarize=binarize, threshold=threshold)\n",
    "\n",
    "    print(\"Saving the Class Datasets\")\n",
    "    torch.save(train_class_loaders, './datasets/train_class_loaders.pth')\n",
    "    torch.save(test_class_loaders, './datasets/test_class_loaders.pth')\n",
    "\n",
    "    # Create TN/TP/FN/FP datasets\n",
    "    general_datasets        = subset.create_tn_tp_fn_fp_datasets(all_images_train, all_labels_train, all_predictions_train)\n",
    "    class_specific_datasets = subset.create_class_specific_datasets(all_images_train, all_labels_train, all_predictions_train)\n",
    "\n",
    "    print(\"Saving the General Datasets\")\n",
    "    torch.save(general_datasets, './datasets/general_datasets.pth')\n",
    "\n",
    "    # For the general TN, TP, FN, FP datasets:\n",
    "    t_dataset = DataLoader(general_datasets['T'],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    f_dataset = DataLoader(general_datasets['F'],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "\n",
    "    print(\"Saving the General Datasets\")\n",
    "    torch.save(t_dataset, './datasets/t_dataset.pth')\n",
    "    torch.save(f_dataset, './datasets/f_dataset.pth')\n",
    "\n",
    "    # For class-specific datasets (e.g., for class 5):\n",
    "    print(\"Saving the Class-Specific Datasets\")\n",
    "    for i in range(10): \n",
    "        class_specific_datasets['TP_class'][i] = DataLoader(class_specific_datasets['TP_class'][i],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "        class_specific_datasets['FP_class'][i] = DataLoader(class_specific_datasets['FP_class'][i],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "        class_specific_datasets['TN_class'][i] = DataLoader(class_specific_datasets['TN_class'][i],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "        class_specific_datasets['FN_class'][i] = DataLoader(class_specific_datasets['FN_class'][i],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "\n",
    "    torch.save(class_specific_datasets, './datasets/class_specific_datasets.pth')    \n",
    "else: \n",
    "    t_dataset               = torch.load('./datasets/t_dataset.pth')\n",
    "    f_dataset               = torch.load('./datasets/f_dataset.pth')\n",
    "    class_specific_datasets = torch.load('./datasets/class_specific_datasets.pth')\n",
    "    train_class_loaders     = torch.load('./datasets/train_class_loaders.pth')\n",
    "    test_class_loaders      = torch.load('./datasets/test_class_loaders.pth')\n",
    "    general_datasets        = torch.load('./datasets/general_datasets.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283f3f3-ef17-4774-a590-02489ad2d667",
   "metadata": {},
   "source": [
    "Class Specific Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cca78da-be99-405f-86d3-2087cc506b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_class_loaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m graphs[g] \u001b[38;5;241m=\u001b[39m LogicGraph(model)\n\u001b[0;32m---> 18\u001b[0m graphs[g]\u001b[38;5;241m.\u001b[39mcompute_sf(\u001b[43mtrain_class_loaders\u001b[49m[g])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_class_loaders' is not defined"
     ]
    }
   ],
   "source": [
    "outputs = [f\"L3_N0\", f\"L3_N1\", f\"L3_N2\", f\"L3_N3\", f\"L3_N4\", f\"L3_N5\", f\"L3_N6\", f\"L3_N7\", f\"L3_N8\", f\"L3_N9\"]\n",
    "\n",
    "p_maps = []\n",
    "sp_graphs = [] \n",
    "sp_diffs = [] \n",
    "for j, output_node in enumerate(outputs): \n",
    "    p_maps.append(cv2.imread(f\"mnist_averages/average_digit_{j}.png\")[4:24,4:24,1]/255)\n",
    "    sp_graph  = copy.deepcopy(gall)\n",
    "    sp_graph.compute_sp(p_maps[j])\n",
    "    sp_graphs.append(sp_graph)\n",
    "    sp_diffs.append(sp_graph.sub_sp(gall))\n",
    "    \n",
    "# Compute SF for each graph\n",
    "graphs = {}\n",
    "for g in range(10): \n",
    "    print(f\"Class {g}\")\n",
    "    graphs[g] = LogicGraph(model)\n",
    "    graphs[g].compute_sf(train_class_loaders[g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0497f639-1ec5-44a7-a244-a1aa222bb954",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_specific_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(g)\n\u001b[1;32m      8\u001b[0m fps[g] \u001b[38;5;241m=\u001b[39m LogicGraph(model)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mclass_specific_datasets\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFP_class\u001b[39m\u001b[38;5;124m'\u001b[39m][g]] \u001b[38;5;241m!=\u001b[39m []:\n\u001b[1;32m     10\u001b[0m     fps[g]\u001b[38;5;241m.\u001b[39mcompute_sf(class_specific_datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFP_class\u001b[39m\u001b[38;5;124m'\u001b[39m][g])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#tps[g] = LogicGraph(model)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#if [i for i in class_specific_datasets['TP_class'][g]] != []:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#    tps[g].compute_sf(class_specific_datasets['TP_class'][g])\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_specific_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute SF for each graph\n",
    "fps = {}\n",
    "tps = {}\n",
    "fns = {}\n",
    "tns = {}\n",
    "for g in range(10): \n",
    "    print(g)\n",
    "    fps[g] = LogicGraph(model)\n",
    "    if [i for i in class_specific_datasets['FP_class'][g]] != []:\n",
    "        fps[g].compute_sf(class_specific_datasets['FP_class'][g])\n",
    "    #tps[g] = LogicGraph(model)\n",
    "    #if [i for i in class_specific_datasets['TP_class'][g]] != []:\n",
    "    #    tps[g].compute_sf(class_specific_datasets['TP_class'][g])\n",
    "    fns[g] = LogicGraph(model)\n",
    "    if [i for i in class_specific_datasets['FN_class'][g]] != []:\n",
    "        fns[g].compute_sf(class_specific_datasets['FN_class'][g])\n",
    "    tns[g] = LogicGraph(model)\n",
    "    if [i for i in class_specific_datasets['TN_class'][g]] != []:\n",
    "        tns[g].compute_sf(class_specific_datasets['TN_class'][g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "747f9edf-9998-474f-963b-555bbe428c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing diffs\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     diff \u001b[38;5;241m=\u001b[39m {} \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m): \n\u001b[0;32m----> 7\u001b[0m         diff[j] \u001b[38;5;241m=\u001b[39m graphs[i] \u001b[38;5;241m-\u001b[39m \u001b[43mgraphs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m     diffs[i] \u001b[38;5;241m=\u001b[39m diff\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing centered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# Compute SF Diff for each combination\n",
    "print(\"Computing diffs\")\n",
    "diffs = {}\n",
    "for i in range(10): \n",
    "    diff = {} \n",
    "    for j in range(10): \n",
    "        diff[j] = graphs[i] - graphs[j]\n",
    "    diffs[i] = diff\n",
    "\n",
    "print(\"Computing centered\")\n",
    "centered = {}\n",
    "for i in range(10): \n",
    "    centered[i] = graphs[i] - gall\n",
    "    \n",
    "print(\"Computing specific\")\n",
    "specific = {}\n",
    "for i in range(10): \n",
    "    specific[i] = graphs[i] - tns[i]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3632c5-07e4-46a4-b877-f7d44bd622fb",
   "metadata": {},
   "source": [
    "Plot the Differences in the switching frequency per Class Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3119c1-d38d-48a9-b1a0-1bfb334db94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10,10,figsize=(10,10))\n",
    "for i in range(10): \n",
    "    for j in range(10): \n",
    "        sf = list(nx.get_node_attributes(diffs[i][j].g,\"sf\").values())\n",
    "        ax[i][j].hist(sf,bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3faee65-87c1-4572-ba02-b7e1ef1da33a",
   "metadata": {},
   "source": [
    "Initialize helper functions for visualizing the FAN-IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c6a0d79-fff6-4e30-a60b-5d1359c1bc59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_not_list = [\"not_implied_by\", \"not_or\",\"not_xor\",\"not_a\",\"implies\",\"not_and\"]\n",
    "b_not_list = [\"not_implies\", \"not_or\",\"not_xor\",\"not_b\",\"implied_by\",\"not_and\"]\n",
    "\n",
    "def find_fan_in(G, output_node, threshold,metric=\"sf\"):\n",
    "    \"\"\"\n",
    "    Find all input nodes connected to the given output node.\n",
    "    \"\"\"\n",
    "    fan_in = set()\n",
    "    nodes_to_check = [(output_node,1)]\n",
    "    input_set = []\n",
    "    while nodes_to_check:\n",
    "        \n",
    "        # Get node and node properties \n",
    "        parent_node, dpo = nodes_to_check.pop(0)\n",
    "        new_fan_in  = []\n",
    "        child_gates = []\n",
    "        parent_type = G.nodes[parent_node][\"gate\"]\n",
    "        \n",
    "        # Account for all predesesors\n",
    "        for child_node in list(G.predecessors(parent_node)): \n",
    "            \n",
    "            desired_parent_output = dpo*1\n",
    "            \n",
    "            # Get type of child node \n",
    "            child_type = G.nodes[child_node][\"gate\"]\n",
    "            \n",
    "            # Account for output case \n",
    "            if parent_type == \"output\": \n",
    "\n",
    "                # Check whether the current node is contributing to the class output\n",
    "                desired_child_output = desired_parent_output\n",
    "                if G.nodes[child_node][metric] > threshold: \n",
    "                    child_gates.append((child_node, desired_child_output))\n",
    "                    new_fan_in.append((child_node,  desired_child_output))\n",
    "\n",
    "            # Account for hidden-layer case \n",
    "            elif child_node != \"input\": \n",
    "\n",
    "                # Get the current wire type\n",
    "                props     = G.get_edge_data(child_node, parent_node)\n",
    "                wire_type = props[\"ab\"] if \"ab\" in props.keys() else None\n",
    "\n",
    "                # Check whether the current wire increases the class score by being zero\n",
    "                if (wire_type == \"a\" and parent_type in a_not_list) or (wire_type == \"b\" and parent_type in b_not_list): \n",
    "                    \n",
    "                    # Adjust the \"desired child output\"\n",
    "                    desired_child_output = -desired_parent_output\n",
    "                    \n",
    "                    # Check whether the current node is contributing to the class output\n",
    "                    if G.nodes[child_node][metric] <= threshold: # Less than because we contribute by being a zero \n",
    "                        child_gates.append((child_node,desired_child_output))\n",
    "                        new_fan_in.append((child_node,desired_child_output))\n",
    "                    \n",
    "                # Case where the current wire increases the class score by being a one\n",
    "                else: \n",
    "                    \n",
    "                    # Adjust the \"desired child output\"\n",
    "                    desired_child_output = desired_parent_output\n",
    "                    \n",
    "                    # Check whether the current node is contributing to the class output\n",
    "                    if G.nodes[child_node][metric] > threshold: \n",
    "                        child_gates.append((child_node,desired_child_output))\n",
    "                        new_fan_in.append((child_node,desired_child_output))\n",
    "\n",
    "            # Account for input case\n",
    "            else: \n",
    "                \n",
    "                # Get the current wire type\n",
    "                props = G.get_edge_data(child_node, parent_node)\n",
    "                wire_type = props[\"ab\"] if \"ab\" in props.keys() else None\n",
    "\n",
    "                # Check whether the current wire increases the class score by being zero\n",
    "                if (wire_type == \"a\" and parent_type in a_not_list) or (wire_type == \"b\" and parent_type in b_not_list): \n",
    "                    \n",
    "                    # Adjust the \"desired child output\"\n",
    "                    child_sign = -1*parent_sign\n",
    "                    new_fan_in.append(  (child_node, child_sign))\n",
    "                    input_set.append( (child_node, child_sign))\n",
    "                    \n",
    "                # Case where the current wire increases the class score by being a one\n",
    "                else: \n",
    "                    \n",
    "                    # Adjust the \"desired child output\"\n",
    "                    child_sign = parent_sign\n",
    "                    new_fan_in.append(  (child_node, child_sign))\n",
    "                    input_set.append( (child_node, child_sign))\n",
    "                        \n",
    "        fan_in.update(new_fan_in)\n",
    "        nodes_to_check.extend(child_gates)\n",
    "\n",
    "    return fan_in, input_set\n",
    "\n",
    "def node_to_pixel(node_id):\n",
    "    \"\"\"\n",
    "    Convert a node ID (e.g., 'L0_Gate 1') to pixel coordinates.\n",
    "    Assumes the input layer corresponds to a 28x28 image (MNIST format).\n",
    "    \"\"\"\n",
    "    if (node_id != \"HIGH\") and (node_id != \"LOW\"): \n",
    "        layer, gate = node_id.split('_')\n",
    "        \n",
    "        gate_num = int(gate.split(\"N\")[1]) - 1  # Convert 'Gate X' to zero-based index\n",
    "\n",
    "        if layer == 'L0':  # Input layer\n",
    "            y = gate_num // 20\n",
    "            x = gate_num % 20\n",
    "            return (x, y)\n",
    "        \n",
    "    return None  # For non-input layers, return None\n",
    "\n",
    "def visualize_fan_in(G, output_node,threshold,signed=True, show=False,metric=\"sf\"):\n",
    "    \"\"\"\n",
    "    Visualize the fan-in of the given output node as highlighted pixels in an image.\n",
    "    \"\"\"\n",
    "    fan_in,pixels = find_fan_in(G, output_node,threshold,metric)\n",
    "    # Create a blank 28x28 image\n",
    "    img = np.zeros((20, 20))\n",
    "    \n",
    "    # Highlight pixels corresponding to input nodes in the fan-in\n",
    "    for node,sign in fan_in:\n",
    "        pixel = node_to_pixel(node)\n",
    "        if pixel:\n",
    "            x, y = pixel\n",
    "            if signed: \n",
    "                img[y, x] += sign  # Set pixel to white (1)\n",
    "            else: \n",
    "                img[y, x] += 1  # Set pixel to white (1)\n",
    "\n",
    "    if show: \n",
    "        # Plot the image\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Fan-in visualization for {output_node}\")\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return img, fan_in,pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a548d62-e58a-4f3d-a757-50e95fd5f4a9",
   "metadata": {},
   "source": [
    "Visualize the saliency maps for the FAN-IN (Class specific SF, and Global SF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54a404eb-a2b3-45f9-bcd9-5e7faecd11ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Uniform 0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m test \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \n\u001b[1;32m     49\u001b[0m gall\u001b[38;5;241m.\u001b[39mcompute_sp(np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m20\u001b[39m))\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mplot_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgall\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgraphs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Average of All MNIST Digits\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m test \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \n\u001b[1;32m     53\u001b[0m p_maps\u001b[38;5;241m.\u001b[39mappend(cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist_averages/average_all_digits.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m24\u001b[39m,\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m24\u001b[39m,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 11\u001b[0m, in \u001b[0;36mplot_row\u001b[0;34m(g, thresholds, signed, metric)\u001b[0m\n\u001b[1;32m      9\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(outputs), figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, output_node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs): \n\u001b[0;32m---> 11\u001b[0m     G \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mg\n\u001b[1;32m     12\u001b[0m     im \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m thresholds: \n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresholds = [float(i)/100-1.0 for i in range(200)]\n",
    "#thresholds = [0.0]\n",
    "outputs = [f\"L3_N0\", f\"L3_N1\", f\"L3_N2\", f\"L3_N3\", f\"L3_N4\", f\"L3_N5\", f\"L3_N6\", f\"L3_N7\", f\"L3_N8\", f\"L3_N9\"]\n",
    "test = False\n",
    "\n",
    "\n",
    "def plot_row(g,thresholds,signed,metric): \n",
    "    # Plot pixel attribution  \n",
    "    fig, ax = plt.subplots(1, len(outputs), figsize=(15, 10))\n",
    "    for i, output_node in enumerate(outputs): \n",
    "        G = g[i].g\n",
    "        im = []\n",
    "        for threshold in thresholds: \n",
    "            img, fan_in,pixels = visualize_fan_in(G, output_node, threshold,signed, False,metric)\n",
    "            im.append(img)\n",
    "        img  = np.average(im,axis=0)\n",
    "        ax[i].imshow(img,cmap=\"inferno\")\n",
    "        ax[i].title.set_text(\" \")#(f\"{i}\")\n",
    "        ax[i].set_axis_off()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"#################################################### MNIST ####################################################\")if test else None \n",
    "import copy \n",
    "fig, ax = plt.subplots(1, len(outputs), figsize=(15, 10))\n",
    "p_maps = []\n",
    "sp_graphs = [] \n",
    "sp_diffs = [] \n",
    "for j, output_node in enumerate(outputs): \n",
    "    p_maps.append(cv2.imread(f\"mnist_averages/average_digit_{j}.png\")[4:24,4:24,1]/255)\n",
    "    sp_graph  = copy.deepcopy(gall)\n",
    "    sp_graph.compute_sp(p_maps[j])\n",
    "    sp_graphs.append(sp_graph)\n",
    "    sp_diffs.append(sp_graph.sub_sp(gall))\n",
    "    ax[j].imshow(p_maps[j])\n",
    "    ax[j].title.set_text(\" \")#(f\"{j}\")\n",
    "    ax[j].set_axis_off()\n",
    "plt.show()\n",
    "#burn = cv2.imread(f\"mnist_averages/average_all_digits.png\")\n",
    "#ax[len(outputs)].imshow(burn[4:24,4:24,1])\n",
    "\n",
    "\n",
    "t1 = [float(i)/100 for i in range(100)]\n",
    "t2 = [float(i)/100-0.5 for i in range(100)]\n",
    "\n",
    "\n",
    "print(\"#################################################### Switch Probability ####################################################\")if test else None \n",
    "\n",
    "print(\"--- Uniform 0.5\") if test else None \n",
    "gall.compute_sp(np.ones((20,20))-0.5)\n",
    "plot_row([gall for i in range(len(graphs))],t1,True,\"sp\")\n",
    "\n",
    "print(\"--- Average of All MNIST Digits\") if test else None \n",
    "p_maps.append(cv2.imread(f\"mnist_averages/average_all_digits.png\")[4:24,4:24,1]/255)\n",
    "gall.compute_sp(p_maps[-1])\n",
    "plot_row([gall for i in range(len(graphs))],t1,True,\"sp\")\n",
    "\n",
    "print(\"--- Average of Class-Specific MNIST Digits\") if test else None \n",
    "plot_row(sp_graphs,t1,True,\"sp\")\n",
    "\n",
    "print(\"--- (Class-Specific-Average) - (Global-Average)\") if test else None \n",
    "plot_row(sp_diffs,t2,True,\"sp\")\n",
    "\n",
    "print(\"#################################################### Switch Frequency ####################################################\")if test else None \n",
    "\n",
    "fig, ax = plt.subplots(1, len(outputs), figsize=(15, 10))\n",
    "p_maps = []\n",
    "sp_graphs = [] \n",
    "sp_diffs = [] \n",
    "for j, output_node in enumerate(outputs): \n",
    "    p_maps.append(cv2.imread(f\"mnist_averages/average_digit_{j}.png\")[4:24,4:24,1]/255)\n",
    "    sp_graph  = copy.deepcopy(gall)\n",
    "    sp_graph.compute_sp(p_maps[j])\n",
    "    sp_graphs.append(sp_graph)\n",
    "    sp_diffs.append(sp_graph.sub_sp(gall))\n",
    "    ax[j].imshow(p_maps[j])\n",
    "    ax[j].title.set_text(\" \")#(f\"{j}\")\n",
    "    ax[j].set_axis_off()\n",
    "plt.show()\n",
    "\n",
    "#print(\"--- Raw Association\") if test else None \n",
    "#plot_row(graphs,[-1],False,\"sf\")\n",
    "\n",
    "print(\"--- (Signed Association)\") if test else None \n",
    "plot_row(graphs,[-1,0],True,\"sf\")\n",
    "\n",
    "print(\"--- (All MNIST)\") if test else None \n",
    "plot_row([gall for i in range(len(graphs))],t1,True,\"sf\")\n",
    "\n",
    "print(\"--- (Class-Specific)\") if test else None \n",
    "plot_row(graphs,t1,True,\"sf\")\n",
    "\n",
    "print(\"--- (Class-Specific) - (All MNIST)\") if test else None \n",
    "plot_row(specific,t2,True,\"sf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82764403-d0b9-47b2-8b97-114951a98cfa",
   "metadata": {},
   "source": [
    "Show and generate local explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92ac54b9-14a8-4706-bce8-fab551978f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'g'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mget_node_attributes(\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg\u001b[49m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m t \u001b[38;5;241m=\u001b[39m [test[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mget_node_attributes(g\u001b[38;5;241m.\u001b[39mg,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mkeys()]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'g'"
     ]
    }
   ],
   "source": [
    "test = nx.get_node_attributes(g.g,\"sf\")\n",
    "t = [test[i] for i in nx.get_node_attributes(g.g,\"sf\").keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "212d5bce-fe25-43bb-af9a-1a0999f9f0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_specific_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Switching Frequency\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,(data,label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mclass_specific_datasets\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFN_class\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m8\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset): \n\u001b[1;32m      3\u001b[0m     g \u001b[38;5;241m=\u001b[39m LogicGraph(model\u001b[38;5;241m.\u001b[39mdiff_logic_model)\n\u001b[1;32m      4\u001b[0m     g\u001b[38;5;241m.\u001b[39mcompute_sf(data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_specific_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Switching Frequency\n",
    "for k,(data,label) in enumerate(class_specific_datasets['FN_class'][8].dataset): \n",
    "    g = LogicGraph(model.diff_logic_model)\n",
    "    g.compute_sf(data)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, len(outputs)+1, figsize=(15, 10))\n",
    "    thresholds = [0.5]\n",
    "    for i, output_node in enumerate(outputs): \n",
    "        im = []\n",
    "        for threshold in thresholds: \n",
    "            img, fan_in,pixels = visualize_fan_in(g.g, output_node, threshold,True,False)\n",
    "            im.append(img)\n",
    "\n",
    "        img  = np.average(im,axis=0)\n",
    "        overlap = np.array(img==(data*2-1).cpu().detach().numpy()).astype(int)\n",
    "        \n",
    "        ax[i].imshow((img*overlap).reshape((20,20,1)))\n",
    "        ax[i].title.set_text(f\"{i}-{np.sum(np.abs(img))}\")\n",
    "    ax[len(outputs)].imshow(data.reshape((20,20,1)))\n",
    "    ax[len(outputs)].title.set_text(f\"{np.argmax(np.array(g.forward(data).cpu().detach()))}\")\n",
    "    plt.show()\n",
    "    \n",
    "    if k > 5: \n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b79c94e-b4ce-49db-b28c-d5f2c7a52472",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_specific_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Switching Probability\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,(data,label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mclass_specific_datasets\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFN_class\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m8\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset): \n\u001b[1;32m      3\u001b[0m     g \u001b[38;5;241m=\u001b[39m LogicGraph(model\u001b[38;5;241m.\u001b[39mdiff_logic_model)\n\u001b[1;32m      4\u001b[0m     g\u001b[38;5;241m.\u001b[39mcompute_sp(data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_specific_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Switching Probability\n",
    "for k,(data,label) in enumerate(class_specific_datasets['FN_class'][8].dataset): \n",
    "    g = LogicGraph(model.diff_logic_model)\n",
    "    g.compute_sp(data)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, len(outputs)+1, figsize=(15, 10))\n",
    "    thresholds = [0]\n",
    "    for i, output_node in enumerate(outputs): \n",
    "        im = []\n",
    "        for threshold in thresholds: \n",
    "            img, fan_in,pixels = visualize_fan_in(g.g, output_node, threshold,True,False,metric=\"sp\")\n",
    "            im.append(img)\n",
    "\n",
    "        img  = np.average(im,axis=0)\n",
    "        overlap = np.array(img==(data*2-1).cpu().detach().numpy()).astype(int)\n",
    "        \n",
    "        ax[i].imshow((img*overlap).reshape((20,20,1)))\n",
    "        ax[i].title.set_text(f\"{i}-{np.sum(np.abs(img))}\")\n",
    "    ax[len(outputs)].imshow(data.reshape((20,20,1)))\n",
    "    ax[len(outputs)].title.set_text(f\"{np.argmax(np.array(g.forward(data).cpu().detach()))}\")\n",
    "    plt.show()\n",
    "    \n",
    "    if k > 5: \n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84627231-8763-4d2b-ba94-6de46d6e1556",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DiffLogic' object has no attribute 'diff_logic_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,(data,label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset): \n\u001b[0;32m----> 3\u001b[0m     g \u001b[38;5;241m=\u001b[39m LogicGraph(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiff_logic_model\u001b[49m)\n\u001b[1;32m      4\u001b[0m     g\u001b[38;5;241m.\u001b[39mcompute_sf(data)\n\u001b[1;32m      6\u001b[0m     fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(outputs)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[0;32m/blue/woodard/mkunzlermaldaner/project/ExpLogic/explogic_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1130\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DiffLogic' object has no attribute 'diff_logic_model'"
     ]
    }
   ],
   "source": [
    "test = True\n",
    "for k,(data,label) in enumerate(train_loader.dataset): \n",
    "    g = LogicGraph(model.diff_logic_model)\n",
    "    g.compute_sf(data)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, len(outputs)+1, figsize=(15, 10))\n",
    "    thresholds = [0.0]\n",
    "    for i, output_node in enumerate(outputs): \n",
    "        im = []\n",
    "        for threshold in thresholds: \n",
    "            img, fan_in,pixels = visualize_fan_in(g.g, output_node, threshold,True,False,test)\n",
    "            im.append(img)\n",
    "\n",
    "        img  = np.average(im,axis=0)\n",
    "        overlap = np.array(img==(data*2-1).cpu().detach().numpy()).astype(int)\n",
    "        \n",
    "        ax[i].imshow((img*overlap).reshape((20,20,1)))\n",
    "        ax[i].title.set_text(f\"{i}-{np.sum(np.abs(img))}\")\n",
    "    ax[len(outputs)].imshow(data.reshape((20,20,1)))\n",
    "    ax[len(outputs)].title.set_text(f\"{np.argmax(np.array(g.forward(data).cpu().detach()))}\")\n",
    "    plt.show()\n",
    "    \n",
    "    if k > 5: \n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96dd2236-2c37-4da7-94ae-31de3f87466e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_specific_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclass_specific_datasets\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFP_class\u001b[39m\u001b[38;5;124m'\u001b[39m][index]\u001b[38;5;241m.\u001b[39mdataset\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_specific_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "class_specific_datasets['FP_class'][index].dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c654762-644b-43ab-9aca-b57b98d3ac1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_specific_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rc\n\u001b[1;32m      3\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,(data,label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mclass_specific_datasets\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTP_class\u001b[39m\u001b[38;5;124m'\u001b[39m][index]\u001b[38;5;241m.\u001b[39mdataset): \n\u001b[1;32m      6\u001b[0m     g \u001b[38;5;241m=\u001b[39m LogicGraph(model\u001b[38;5;241m.\u001b[39mdiff_logic_model)\n\u001b[1;32m      7\u001b[0m     g\u001b[38;5;241m.\u001b[39mcompute_sf(data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_specific_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import rc\n",
    "\n",
    "index = 1\n",
    "\n",
    "for k,(data,label) in enumerate(class_specific_datasets['TP_class'][index].dataset): \n",
    "    g = LogicGraph(model.diff_logic_model)\n",
    "    g.compute_sf(data)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, len(outputs)+1, figsize=(15, 10))\n",
    "    thresholds = [0.0]\n",
    "    for i, output_node in enumerate(outputs): \n",
    "        im = []\n",
    "        for threshold in thresholds: \n",
    "            img, fan_in,pixels = visualize_fan_in(g.g, output_node, threshold,True,False)\n",
    "            im.append(img)\n",
    "\n",
    "        img  = np.average(im,axis=0)\n",
    "        overlap = np.array(img==(data*2-1).cpu().detach().numpy()).astype(int)\n",
    "        \n",
    "        ax[i+1].imshow((img*overlap).reshape((20,20,1)),\"inferno\",vmin=-1,vmax=1)\n",
    "        ax[i+1].title.set_text(f\"C: {i}; $\\Sigma$={int(np.sum(np.abs(img)*overlap))}\")\n",
    "        \n",
    "        ax[i+1].set_axis_off()\n",
    "        \n",
    "    ax[0].imshow(data.reshape((20,20,1)),\"inferno\")\n",
    "    ax[0].title.set_text(f\"C: {np.argmax(np.array(g.forward(data).cpu().detach()))} (TP)\")\n",
    "    ax[0].set_axis_off()\n",
    "    plt.show()\n",
    "    \n",
    "    break \n",
    "        \n",
    "for k,(data,label) in enumerate(class_specific_datasets['FP_class'][index].dataset): \n",
    "    g = LogicGraph(model.diff_logic_model)\n",
    "    g.compute_sf(data)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, len(outputs)+1, figsize=(15, 10))\n",
    "    thresholds = [0.0]\n",
    "    for i, output_node in enumerate(outputs): \n",
    "        im = []\n",
    "        for threshold in thresholds: \n",
    "            img, fan_in,pixels = visualize_fan_in(g.g, output_node, threshold,True,False)\n",
    "            im.append(img)\n",
    "\n",
    "        img  = np.average(im,axis=0)\n",
    "        overlap = np.array(img==(data*2-1).cpu().detach().numpy()).astype(int)\n",
    "        \n",
    "        ax[i+1].imshow((img*overlap).reshape((20,20,1)),\"inferno\",vmin=-1,vmax=1)\n",
    "        ax[i+1].title.set_text(f\"C: {i}; $\\Sigma$={int(np.sum(np.abs(img)*overlap))}\")\n",
    "        \n",
    "        ax[i+1].set_axis_off()\n",
    "        \n",
    "    ax[0].imshow(data.reshape((20,20,1)),\"inferno\")\n",
    "    ax[0].title.set_text(f\"C: {np.argmax(np.array(g.forward(data).cpu().detach()))} (FP)\")\n",
    "    ax[0].set_axis_off()\n",
    "    plt.show()\n",
    "    \n",
    "    break \n",
    "    \n",
    "        \n",
    "for k,(data,label) in enumerate(class_specific_datasets['FN_class'][index].dataset): \n",
    "    g = LogicGraph(model.diff_logic_model)\n",
    "    g.compute_sf(data)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, len(outputs)+1, figsize=(15, 10))\n",
    "    thresholds = [0.0]\n",
    "    for i, output_node in enumerate(outputs): \n",
    "        im = []\n",
    "        for threshold in thresholds: \n",
    "            img, fan_in,pixels = visualize_fan_in(g.g, output_node, threshold,True,False)\n",
    "            im.append(img)\n",
    "\n",
    "        img  = np.average(im,axis=0)\n",
    "        overlap = np.array(img==(data*2-1).cpu().detach().numpy()).astype(int)\n",
    "        \n",
    "        ax[i+1].imshow((img*overlap).reshape((20,20,1)),\"inferno\")\n",
    "        ax[i+1].title.set_text(f\"C: {i}; $\\Sigma$={int(np.sum(np.abs(img*overlap)))}\")\n",
    "        \n",
    "        ax[i+1].set_axis_off()\n",
    "        \n",
    "    ax[0].imshow(data.reshape((20,20,1)),\"inferno\")\n",
    "    ax[0].title.set_text(f\"C: {np.argmax(np.array(g.forward(data).cpu().detach()))} (FN)\")\n",
    "    ax[0].set_axis_off()\n",
    "    plt.show()\n",
    "    \n",
    "    break     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fb989-c76f-44fe-9103-af4219cb52ee",
   "metadata": {},
   "source": [
    "#### **Regions associated with FN and FP cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca4428-e69b-4e68-a620-5cc724b6f10e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c0ef565-a0b6-4dbf-a8b8-adcaf386825a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Hardware**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c06262-afe8-4b02-883c-c044a060fc39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Verilog Conversion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e20496-c3ac-4d87-8e55-359ac0069b03",
   "metadata": {},
   "source": [
    "Logic gate to Verilog expression mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846e8b8-9b6f-49dc-adbc-7bfe8fe0a27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logic_gate_verilog = {\n",
    "    \"0\": \"1'b0\",\n",
    "    \"A∧B\": \"({a}) & ({b})\",\n",
    "    \"¬(A⇒B)\": \"({a}) & ~({b})\",\n",
    "    \"A\": \"{a}\",\n",
    "    \"¬(B⇒A)\": \"({b}) & ~({a})\",\n",
    "    \"B\": \"{b}\",\n",
    "    \"A⊕B\": \"({a}) ^ ({b})\",\n",
    "    \"A∨B\": \"({a}) | ({b})\",\n",
    "    \"¬(A∨B)\": \"~(({a}) | ({b}))\",\n",
    "    \"¬(A⊕B)\": \"~(({a}) ^ ({b}))\",\n",
    "    \"¬B\": \"~({b})\",\n",
    "    \"B⇒A\": \"~({b}) | ({a})\",\n",
    "    \"¬A\": \"~({a})\",\n",
    "    \"A⇒B\": \"~({a}) | ({b})\",\n",
    "    \"¬(A∧B)\": \"~(({a}) & ({b}))\",\n",
    "    \"1\": \"1'b1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a28141-16b1-4e15-8adc-a4aa7cb405cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "side = 16 # pixels in one side of the image\n",
    "N_input = 16 * 16  # number of input dimensions\n",
    "\n",
    "# converts the learned logic gates to verilog or vhdl \n",
    "def generate_verilog(model, filename=\"logic_network.v\"):\n",
    "    \n",
    "    N_layers = len(model.logic_layers)\n",
    "\n",
    "    # gets number of neurons per layer\n",
    "    neurons_per_layer = [layer.weights.size()[0] for layer in model.logic_layers]\n",
    "    \n",
    "    # Set the output size to the number of neurons in the last layer\n",
    "    N_output = neurons_per_layer[-1]\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        # module declaration\n",
    "        file.write(\"module logic_network(\\n\")\n",
    "        file.write(f\"    input wire [{N_input-1}:0] inputs,\\n\")\n",
    "        file.write(f\"    output wire [{N_output-1}:0] outputs\\n\")\n",
    "        file.write(\");\\n\\n\")\n",
    "\n",
    "        # declares wires for internal layers\n",
    "        for layer_index in range(N_layers - 1):\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "            file.write(f\"    wire [{N_neurons -1}:0] layer{layer_index}_outputs;\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "        logic_operations = list(logic_gate_verilog.keys())\n",
    "\n",
    "        for layer_index in range(N_layers):\n",
    "            logic_layer = model.logic_layers[layer_index]\n",
    "\n",
    "            # gets input and output indices\n",
    "            input_indices = logic_layer.indices[0].cpu().numpy()  # first input indices\n",
    "            output_indices = logic_layer.indices[1].cpu().numpy()  # second input indices\n",
    "\n",
    "            neuron_gates = [torch.argmax(logic_layer.weights[neuron]).item()\n",
    "                            for neuron in range(logic_layer.weights.size()[0])]\n",
    "            connections = {i: (input_indices[i], output_indices[i]) for i in range(len(neuron_gates))}\n",
    "\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "\n",
    "            # determines input wires\n",
    "            if layer_index == 0:\n",
    "                input_wire_base = \"inputs\"\n",
    "            else:\n",
    "                input_wire_base = f\"layer{layer_index -1}_outputs\"\n",
    "\n",
    "            # determines output wires\n",
    "            if layer_index == N_layers - 1:\n",
    "                output_wire_base = \"outputs\"\n",
    "            else:\n",
    "                output_wire_base = f\"layer{layer_index}_outputs\"\n",
    "\n",
    "            # assign statements for this layer\n",
    "            for neuron_id in range(N_neurons):\n",
    "                a_idx, b_idx = connections[neuron_id]\n",
    "\n",
    "                # maps indices to input wires\n",
    "                a_wire = f\"{input_wire_base}[{a_idx}]\"\n",
    "                b_wire = f\"{input_wire_base}[{b_idx}]\"\n",
    "\n",
    "                # gets gate\n",
    "                gate_op = logic_operations[neuron_gates[neuron_id]]\n",
    "                gate = logic_gate_verilog[gate_op].format(a=a_wire, b=b_wire)\n",
    "\n",
    "                # assigns to output wire\n",
    "                output_wire = f\"{output_wire_base}[{neuron_id}]\"\n",
    "\n",
    "                file.write(f\"    assign {output_wire} = {gate};\\n\")\n",
    "\n",
    "        file.write(\"endmodule\\n\")\n",
    "        print('success')\n",
    "\n",
    "# generates Verilog file for all trained models\n",
    "for model_idx in range(len(trained_models)):\n",
    "    i = model_idx + 1\n",
    "    generate_verilog(trained_models[model_idx], filename=f\"verilog/{side}x{side}/model_{i:03d}_logic_network.v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7325be-1e5f-4c16-8a49-a66327d183bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **VHDL Conversion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22f5f2-059b-453e-b32e-9634cf8a9e3c",
   "metadata": {},
   "source": [
    "Logic gate to Verilog expression mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2f39a-e420-4b4d-93a6-1c0182bd15b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logic_gate_vhdl = {\n",
    "    \"0\": \"'0'\",\n",
    "    \"A∧B\": \"({a}) and ({b})\",\n",
    "    \"¬(A⇒B)\": \"({a}) and not ({b})\",\n",
    "    \"A\": \"{a}\",\n",
    "    \"¬(B⇒A)\": \"({b}) and not ({a})\",\n",
    "    \"B\": \"{b}\",\n",
    "    \"A⊕B\": \"({a}) xor ({b})\",\n",
    "    \"A∨B\": \"({a}) or ({b})\",\n",
    "    \"¬(A∨B)\": \"not(({a}) or ({b}))\",\n",
    "    \"¬(A⊕B)\": \"not(({a}) xor ({b}))\",\n",
    "    \"¬B\": \"not({b})\",\n",
    "    \"B⇒A\": \"not({b}) or ({a})\",\n",
    "    \"¬A\": \"not({a})\",\n",
    "    \"A⇒B\": \"not({a}) or ({b})\",\n",
    "    \"¬(A∧B)\": \"not(({a}) and ({b}))\",\n",
    "    \"1\": \"'1'\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20389672-5297-4dd4-a368-1b96df1445ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "side = 16 # pixels in one side of the image\n",
    "N_input = 16 * 16  # number of input dimensions\n",
    "\n",
    "# Converts the learned logic gates to VHDL\n",
    "def generate_vhdl(model, filename=\"logic_network.vhdl\"):\n",
    "    N_layers = len(model.logic_layers)\n",
    "    neurons_per_layer = [layer.weights.size()[0] for layer in model.logic_layers]\n",
    "\n",
    "    # set output size to the number of neurons in the last layer\n",
    "    N_output = neurons_per_layer[-1]\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        # Library and entity declaration\n",
    "        file.write(\"library IEEE;\\n\")\n",
    "        file.write(\"use IEEE.STD_LOGIC_1164.ALL;\\n\\n\")\n",
    "        file.write(\"entity logic_network is\\n\")\n",
    "        file.write(f\"    port (\\n\")\n",
    "        file.write(f\"        inputs : in std_logic_vector({N_input - 1} downto 0);\\n\")\n",
    "        file.write(f\"        outputs : out std_logic_vector({N_output - 1} downto 0)\\n\")\n",
    "        file.write(\"    );\\n\")\n",
    "        file.write(\"end logic_network;\\n\\n\")\n",
    "        file.write(\"architecture Behavioral of logic_network is\\n\")\n",
    "\n",
    "        # Declare signals for internal layers\n",
    "        for layer_index in range(N_layers - 1):\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "            file.write(f\"    signal layer{layer_index}_outputs : std_logic_vector({N_neurons - 1} downto 0);\\n\")\n",
    "        file.write(\"\\nbegin\\n\\n\")\n",
    "\n",
    "        logic_operations = list(logic_gate_vhdl.keys())\n",
    "\n",
    "        # Generate VHDL code for each layer\n",
    "        for layer_index in range(N_layers):\n",
    "            logic_layer = model.logic_layers[layer_index]\n",
    "\n",
    "            # Get input and output indices\n",
    "            input_indices = logic_layer.indices[0].cpu().numpy()  # first input indices\n",
    "            output_indices = logic_layer.indices[1].cpu().numpy()  # second input indices\n",
    "\n",
    "            neuron_gates = [torch.argmax(logic_layer.weights[neuron]).item()\n",
    "                            for neuron in range(logic_layer.weights.size()[0])]\n",
    "            connections = {i: (input_indices[i], output_indices[i]) for i in range(len(neuron_gates))}\n",
    "\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "\n",
    "            # Determine input signals\n",
    "            if layer_index == 0:\n",
    "                input_wire_base = \"inputs\"\n",
    "            else:\n",
    "                input_wire_base = f\"layer{layer_index -1}_outputs\"\n",
    "\n",
    "            # Determine output signals\n",
    "            if layer_index == N_layers - 1:\n",
    "                output_wire_base = \"outputs\"\n",
    "            else:\n",
    "                output_wire_base = f\"layer{layer_index}_outputs\"\n",
    "\n",
    "            # Assign statements for each neuron in this layer\n",
    "            for neuron_id in range(N_neurons):\n",
    "                a_idx, b_idx = connections[neuron_id]\n",
    "\n",
    "                # Map indices to input signals\n",
    "                a_wire = f\"{input_wire_base}({a_idx})\"\n",
    "                b_wire = f\"{input_wire_base}({b_idx})\"\n",
    "\n",
    "                # Get the gate operation\n",
    "                gate_op = logic_operations[neuron_gates[neuron_id]]\n",
    "                gate = logic_gate_vhdl[gate_op].format(a=a_wire, b=b_wire)\n",
    "\n",
    "                # Assign to output signal\n",
    "                output_wire = f\"{output_wire_base}({neuron_id})\"\n",
    "                file.write(f\"    {output_wire} <= {gate};\\n\")\n",
    "\n",
    "        file.write(\"\\nend Behavioral;\\n\")\n",
    "        print('success')\n",
    "\n",
    "# Generates VHDL files for all trained models\n",
    "for model_idx in range(len(trained_models)):\n",
    "    i = model_idx + 1\n",
    "    generate_vhdl(trained_models[model_idx], filename=f\"vhdl/{side}x{side}/model_{i:03d}_logic_network.vhdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3597-9cf6-4688-a0ab-a676848f4439",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Predicting from Hex**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59df202-6740-4136-82ad-2ad266c20eb6",
   "metadata": {},
   "source": [
    "Sanity check to see that the FPGA output matches the model predictions on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4a4f5-4b7f-47be-bf77-6864016839c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def load_images_from_hex_file(filename):\n",
    "    images = []\n",
    "    hex_pattern = re.compile(r'^[0-9a-fA-F]+$')  # only allows valid hex characters\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if ':' in line:\n",
    "                hex_data = line.split(':')[1].strip().strip(';')\n",
    "                \n",
    "                # check if hex_data is a valid hexadecimal string\n",
    "                if hex_pattern.match(hex_data):\n",
    "                    # convert to binary and pad to 256 bits\n",
    "                    bin_data = bin(int(hex_data, 16))[2:].zfill(256)\n",
    "                    image = np.array([int(bit) for bit in bin_data], dtype=np.uint8).reshape(16, 16)\n",
    "                    images.append(image)\n",
    "                else:\n",
    "                    print(f\"Skipping invalid line: {line.strip()}\")\n",
    "\n",
    "    return np.array(images)\n",
    "\n",
    "# loading images\n",
    "filename = 'mnist_input.mif'  \n",
    "images = load_images_from_hex_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7da784-e8c6-47b5-af6f-4c8b09e1dcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference_on_images(model, images):\n",
    "    model.eval()  # sets model to evaluation mode\n",
    "    predictions = []\n",
    "    \n",
    "    # starts inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for img in images:\n",
    "        # flatten each 16x16 image to a 1D tensor with 256 elements\n",
    "        input_data = torch.tensor(img.flatten(), dtype=torch.float32).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        with torch.no_grad():  # disable gradient computation for inference\n",
    "            output = model(input_data)\n",
    "            _, predicted_label = torch.max(output, 1)  # get the predicted label\n",
    "            predictions.append(predicted_label.item())\n",
    "    \n",
    "    # end timing inference\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time  # total inference time in seconds\n",
    "    avg_inference_time = inference_time / len(images)\n",
    "    \n",
    "    return predictions, avg_inference_time\n",
    "\n",
    "# dictionary to store the predictions and avg inference times for each of the 25 models    \n",
    "predictions = {}\n",
    "avg_inference_times = {}\n",
    "\n",
    "for model_idx in trained_models:\n",
    "    prediction, avg_inference_time = run_inference_on_images(trained_models[model_idx], images)\n",
    "    predictions[model_idx] = prediction\n",
    "    avg_inference_times[model_idx] = avg_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d6ebd-1029-41b0-945b-e255cca2ec6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from difflogic import CompiledLogicNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689d378-2324-496d-bcb2-6d06d5220c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference_with_packbits(model, images):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    # Start timing the inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop through each image\n",
    "    with torch.no_grad():  # No gradient computation needed\n",
    "        for img in images:\n",
    "            # Convert image to boolean tensor and flatten\n",
    "            print(img)\n",
    "            img_tensor = torch.tensor(img, dtype=torch.bool).view(1, -1)  # Shape: [1, flattened_dim]\n",
    "            print(img_tensor)\n",
    "                \n",
    "            # Convert to PackBitsTensor for efficient inference\n",
    "            packed_input = difflogic.PackBitsTensor(img_tensor)\n",
    "            print(packed_input)\n",
    "            \n",
    "            # Perform inference with PackBitsTensor\n",
    "            output = model(packed_input)\n",
    "\n",
    "            # Get predicted class\n",
    "            _, predicted_label = torch.max(output, 1)\n",
    "            predictions.append(predicted_label.item())\n",
    "\n",
    "    # End timing the inference\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time  # Total inference time in seconds\n",
    "    avg_inference_time = inference_time / len(images)\n",
    "\n",
    "    return predictions, avg_inference_time\n",
    "\n",
    "# Assuming `images` is a numpy array of shape [num_images, height, width]\n",
    "predictions = {}\n",
    "avg_inference_times = {}\n",
    "\n",
    "model = trained_models\n",
    "model.implementation = 'cuda'\n",
    "for model_idx, trained_model in trained_models.items():\n",
    "    prediction, avg_inference_time = run_inference_with_packbits(trained_model, images)\n",
    "    predictions[model_idx] = prediction\n",
    "    avg_inference_times[model_idx] = avg_inference_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca015aa-a294-4600-a80b-d4c9e41438d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# displays some sample images with predictions\n",
    "def display_images_side_by_side(images, predictions, num_images=10, title=\"Model\", accuracy=None, avg_inference_time=None):\n",
    "    \n",
    "    # converts avg_inference_time from seconds to milliseconds\n",
    "    avg_inference_time_ms = avg_inference_time * 1000 if avg_inference_time is not None else 0\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 2))  \n",
    "    \n",
    "    # sets big title above all images with accuracy and inference time\n",
    "    fig.suptitle(f\"{title}\\nAccuracy: {accuracy:.2%} | Avg Inference Time: {avg_inference_time_ms:.2f} ms\", \n",
    "                 fontsize=14, fontweight='bold', y=1.1)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(images[i], cmap='gray')\n",
    "        axes[i].set_title(f\"Pred: {predictions[i]}\")\n",
    "        axes[i].axis('off')  \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# loops through each model and display images with respective accuracy and inference time\n",
    "for model_name, preds in predictions.items():\n",
    "    # gets accuracy and inference time for the current model\n",
    "    accuracy = trained_models_accuracies[model_name]\n",
    "    avg_inference_time = avg_inference_times[model_name]\n",
    "    # displaying the first 10 images side by side with their predictions\n",
    "    display_images_side_by_side(\n",
    "        images, preds, num_images=10, \n",
    "        title=f'Model {model_name}', \n",
    "        accuracy=accuracy, \n",
    "        avg_inference_time=avg_inference_time\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExpLogic",
   "language": "python",
   "name": "difflogic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
