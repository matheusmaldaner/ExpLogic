{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267af5dd-e83a-4431-8036-c75321cb3945",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **ExpLogic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568cabd-3e11-474b-af13-f6c0c668454d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Setting up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65bb0b-594d-4717-bd3c-63b04b9d864b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb5dd1c0-73cf-4679-ae21-6fa00a85a395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import yaml\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Compose, Lambda\n",
    "import mnist_dataset\n",
    "from hydra import initialize, compose\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n",
    "from difflogic import LogicLayer, GroupSum, PackBitsTensor\n",
    "from mnist_dataset import MNISTRemoveBorderTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99c41f-b5c7-4d64-a704-5a003a5b56c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8096d-fc17-46d7-861e-685d2fe12dbb",
   "metadata": {},
   "source": [
    "Tunable Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ea8409-a084-4bee-8121-96acd99bb28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configurable options\n",
    "remove_border = True       # True: Removes border of Mnist, False: Keeps black border around digits\n",
    "binarize_images = True     # True: Binarized Images, False: Grayscale Images \n",
    "evenly_partitioned = True  # True: Even distribution of samples, False: Original Mnist distribution\n",
    "upscaled_images = False    # True: Upscales the samples to 32x32, False: Keeps size unchanged\n",
    "downscaled_images = False   # True: Downscales the samples to 16x16, False: Keeps size unchanged\n",
    "batch_size = 256           # Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63667e4-7675-4c59-a6f3-18f4acd21475",
   "metadata": {},
   "source": [
    "Set Seed for Reproducibiity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe299cc0-4872-4902-94f7-b85c193bd43f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)      # PyTorch seed fixing\n",
    "torch.cuda.manual_seed(42) # PyTorch CUDA seed fixing (if using GPU)\n",
    "np.random.seed(42)         # NumPy seed fixing\n",
    "random.seed(42)            # Python's built-in random seed fixing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4450630-e7a8-4564-b106-6e2dc3a12e61",
   "metadata": {},
   "source": [
    "Dataset Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1779afcd-2def-41d1-abdc-c1bc8dbb1719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to binarize an image, threshold is tunable \n",
    "def binarize(image, threshold=0.5):\n",
    "    return (image > threshold).float()  \n",
    "\n",
    "# define the transformation logic based on the toggle\n",
    "transform_list = [ToTensor()]\n",
    "\n",
    "# removes border around each mnist digit\n",
    "if remove_border:\n",
    "    transform_list.append(MNISTRemoveBorderTransform())\n",
    "\n",
    "# upscales or downscales the images\n",
    "if upscaled_images:\n",
    "    transform_list.append(Resize((32, 32)))\n",
    "elif downscaled_images:\n",
    "    transform_list.append(Resize((16, 16)))\n",
    "\n",
    "# binarizes the images\n",
    "if binarize_images:\n",
    "    transform_list.append(Lambda(lambda x: binarize(x)))\n",
    "    \n",
    "transform = Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d9255d2-bf0c-4bab-b66a-b65360bcb86f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = mnist_dataset.MNIST('./data-mnist', train=True, download=True, transform=transform)\n",
    "test_dataset = mnist_dataset.MNIST('./data-mnist', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a5db70-d2ba-44a7-803c-829177a182c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes the Dataset evenly partitioned\n",
    "if evenly_partitioned:\n",
    "    # code below is used so that all classes have the same number of samples\n",
    "    train_targets = train_loader.dataset.targets\n",
    "    test_targets = test_loader.dataset.targets\n",
    "\n",
    "    train_digits_total = []\n",
    "    test_digits_total = []\n",
    "\n",
    "    for i in range(10):\n",
    "        curr_tot_train = torch.sum(train_targets == i).item()\n",
    "        curr_tot_test = torch.sum(test_targets == i).item()    \n",
    "        train_digits_total.append(curr_tot_train)\n",
    "        test_digits_total.append(curr_tot_test)\n",
    "\n",
    "    train_digits_total, test_digits_total\n",
    "\n",
    "    # find the minimum number of samples across all classes\n",
    "    min_samples_train = min(train_digits_total)\n",
    "    min_samples_test = min(test_digits_total)\n",
    "\n",
    "    # function to trim dataset to match the minimum samples for each class and shuffle indices\n",
    "    def trim_dataset(dataset, targets, min_samples):\n",
    "        indices = []\n",
    "        for i in range(10):\n",
    "            class_indices = (targets == i).nonzero(as_tuple=True)[0]  # get indices of class i\n",
    "            class_indices = class_indices[:min_samples]  # trim to min_samples\n",
    "            indices.extend(class_indices)\n",
    "\n",
    "        # shuffle indices after collecting them\n",
    "        indices = torch.tensor(indices)\n",
    "        indices = indices[torch.randperm(indices.size(0))]  \n",
    "\n",
    "        return Subset(dataset, indices)\n",
    "\n",
    "    # trim both train and test datasets to ensure all classes have the same number of samples\n",
    "    trimmed_train_dataset = trim_dataset(train_loader.dataset, train_targets, min_samples_train)\n",
    "    trimmed_test_dataset = trim_dataset(test_loader.dataset, test_targets, min_samples_test)\n",
    "\n",
    "    # create DataLoaders for the trimmed datasets\n",
    "    trimmed_train_loader = DataLoader(trimmed_train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    trimmed_test_loader = DataLoader(trimmed_test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # verify the lengths of the trimmed datasets\n",
    "    len(trimmed_train_loader.dataset), len(trimmed_test_loader.dataset)\n",
    "\n",
    "    train_dataset = trimmed_train_dataset\n",
    "    test_dataset = trimmed_test_dataset\n",
    "    train_loader = trimmed_train_loader\n",
    "    test_loader = trimmed_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66230413-408e-4bc1-bf5f-2313690e0964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14c758964f10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGdCAYAAABKG5eZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjGklEQVR4nO3df2xV9f3H8dcp0Ftm6O0c0PZKKT8moAidI2stm/M76Sidw7IfioRpmYqGlGSGuSCLrmz7o06MfwwbNJtQFzd/JaMk6tig8kOgiKM0IpqGstpC4JZI5Ny2SEvaz/ePyZ133Nv2jnt77+f2+Ujeieecz/n0fT/ew4t77+HWMcYYAQBgibRENwAAQDQILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVUYnuoFY6O/v1+nTpzVu3Dg5jpPodgAAUTLGqLOzUz6fT2lpA7+mSongOn36tPLy8hLdBgDgKp08eVKTJk0acExKBNe4ceMS3QIwIrmuG5N5vF5vTOaB/Yby53lKBBdvDwKJkZmZmegWkGKG8uc5N2cAAKxCcAEArBK34KqpqdGUKVOUkZGhoqIiHTp0aMDxr7/+umbNmqWMjAzNmTNHb731VrxaAwDYzMTBK6+8YtLT083mzZvNsWPHzMqVK01WVpbp6OgIO37//v1m1KhR5qmnnjIffvihefzxx82YMWPM0aNHh/TzXNc1kiiKGuaKlUQ/Dip5ynXdwZ8vMXvmfUFhYaGprKwMbvf19Rmfz2eqq6vDjr/77rvNHXfcEbKvqKjIPPzww0P6eQQXRSWmYiXRj4NKnhpKcMX8rcLe3l4dPnxYJSUlwX1paWkqKSlRQ0ND2HMaGhpCxktSaWlpxPE9PT0KBAIhBQAYGWIeXJ988on6+vqUnZ0dsj87O1t+vz/sOX6/P6rx1dXV8nq9weIfHwPAyGHlXYXr1q2T67rBOnnyZKJbAgAMk5j/A+Tx48dr1KhR6ujoCNnf0dGhnJycsOfk5ORENd7j8cjj8cSmYQCAVWL+iis9PV3z5s1TfX19cF9/f7/q6+tVXFwc9pzi4uKQ8ZK0Y8eOiOMBACNYzG4L+oJXXnnFeDweU1tbaz788EPz0EMPmaysLOP3+40xxtx7773mscceC47fv3+/GT16tHn66afNRx99ZKqqqrgdnqIsqFhJ9OOgkqcSdju8McZs3LjRTJ482aSnp5vCwkJz8ODB4LHbbrvNVFRUhIx/7bXXzIwZM0x6erqZPXu2efPNN4f8swguikpMxUqiHweVPDWU4HI+f9JYLRAI8O3SQALE6o8Pvigbl7muO+iXN1t5VyEAYORKiV9rAmDoUuBNFoxwvOICAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYJebBVV1drW984xsaN26cJk6cqCVLlqi5uXnAc2pra+U4TkhlZGTEujUAQAqIeXDt2bNHlZWVOnjwoHbs2KFLly5p4cKF6u7uHvC8zMxMnTlzJlhtbW2xbg0AkAJGx3rC7du3h2zX1tZq4sSJOnz4sL797W9HPM9xHOXk5MS6HQBAion7Z1yu60qSrr322gHHdXV1KT8/X3l5eSovL9exY8ciju3p6VEgEAgpINUZY2JSgO3iGlz9/f165JFH9M1vflM33XRTxHEzZ87U5s2btW3bNr300kvq7+/X/PnzderUqbDjq6ur5fV6g5WXlxevhwAASDKOieNfwVatWqW//e1v2rdvnyZNmjTk8y5duqQbbrhBy5Yt029/+9srjvf09Kinpye4HQgECC+kvFR+teQ4TqJbQJJwXVeZmZkDjon5Z1yXrV69Wm+88Yb27t0bVWhJ0pgxY3TzzTerpaUl7HGPxyOPxxOLNgEAlon5W4XGGK1evVpbt27V22+/ralTp0Y9R19fn44eParc3NxYtwcAsFzMX3FVVlbqL3/5i7Zt26Zx48bJ7/dLkrxer8aOHStJuu+++3TdddepurpakvSb3/xGt9xyi7761a/q/Pnz2rBhg9ra2vTggw/Guj0AgOViHlybNm2SJP3f//1fyP4tW7ZoxYoVkqT29nalpf3nxd6nn36qlStXyu/368tf/rLmzZunAwcO6MYbb4x1ewAAy8X15ozhEggE5PV6E90GEFcpcKlGxM0ZuGwoN2fwXYUAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAq8Tt93EBSH18xyASgVdcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKvEPLjWr18vx3FCatasWQOe8/rrr2vWrFnKyMjQnDlz9NZbb8W6LQBAiojLK67Zs2frzJkzwdq3b1/EsQcOHNCyZcv0wAMP6MiRI1qyZImWLFmiDz74IB6tAQAs5xhjTCwnXL9+verq6tTU1DSk8UuXLlV3d7feeOON4L5bbrlFX/va1/Tcc88NaY5AICCv1/u/tAtYI8aXakw4jpPoFpBiXNdVZmbmgGPi8orr+PHj8vl8mjZtmpYvX6729vaIYxsaGlRSUhKyr7S0VA0NDRHP6enpUSAQCCkAwMgQ8+AqKipSbW2ttm/frk2bNqm1tVW33nqrOjs7w473+/3Kzs4O2ZednS2/3x/xZ1RXV8vr9QYrLy8vpo8BAJC8Yh5cZWVluuuuuzR37lyVlpbqrbfe0vnz5/Xaa6/F7GesW7dOrusG6+TJkzGbGwCQ3EbH+wdkZWVpxowZamlpCXs8JydHHR0dIfs6OjqUk5MTcU6PxyOPxxPTPgEAdoj7v+Pq6urSiRMnlJubG/Z4cXGx6uvrQ/bt2LFDxcXF8W4NAGAjE2M///nPze7du01ra6vZv3+/KSkpMePHjzdnz541xhhz7733msceeyw4fv/+/Wb06NHm6aefNh999JGpqqoyY8aMMUePHh3yz3Rd10iiqJSuZJToNaFSr1zXHfR5F/O3Ck+dOqVly5bp3LlzmjBhgr71rW/p4MGDmjBhgiSpvb1daWn/eaE3f/58/eUvf9Hjjz+uX/7yl7r++utVV1enm266KdatAQBSQMz/HVci8O+4MBIk46XKv+NCrCXs33EBABAvBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoxD64pU6bIcZwrqrKyMuz42traK8ZmZGTEui0AQIoYHesJ33vvPfX19QW3P/jgA333u9/VXXfdFfGczMxMNTc3B7cdx4l1WwCAFBHz4JowYULI9pNPPqnp06frtttui3iO4zjKycmJdSsAgBQU18+4ent79dJLL+n+++8f8FVUV1eX8vPzlZeXp/Lych07diyebQEALBbzV1xfVFdXp/Pnz2vFihURx8ycOVObN2/W3Llz5bqunn76ac2fP1/Hjh3TpEmTwp7T09Ojnp6e4HYgEIh160BMGGMS3cIVeCse1jNxtHDhQvP9738/qnN6e3vN9OnTzeOPPx5xTFVVlZFEUUlfySjRa0JRA5XruoM+h+P2VmFbW5t27typBx98MKrzxowZo5tvvlktLS0Rx6xbt06u6wbr5MmTV9suAMAScQuuLVu2aOLEibrjjjuiOq+vr09Hjx5Vbm5uxDEej0eZmZkhBQAYGeISXP39/dqyZYsqKio0enTox2j33Xef1q1bF9z+zW9+o3/84x/617/+pcbGRv3kJz9RW1tb1K/UAAAjQ1xuzti5c6fa29t1//33X3Gsvb1daWn/yctPP/1UK1eulN/v15e//GXNmzdPBw4c0I033hiP1gAAlnM+/7DWaoFAQF6vN9FtAFdIxsuLuwqRzFzXHfTjH76rEABgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYBWCCwBgFYILAGAVggsAYJXRiW4ASEbGmES3cAXHcRLdApAUeMUFALAKwQUAsArBBQCwCsEFALAKwQUAsErUwbV3714tXrxYPp9PjuOorq4u5LgxRr/61a+Um5ursWPHqqSkRMePHx903pqaGk2ZMkUZGRkqKirSoUOHom0NADACRB1c3d3dKigoUE1NTdjjTz31lH7/+9/rueee07vvvqtrrrlGpaWlunjxYsQ5X331Va1Zs0ZVVVVqbGxUQUGBSktLdfbs2WjbAwCkOnMVJJmtW7cGt/v7+01OTo7ZsGFDcN/58+eNx+MxL7/8csR5CgsLTWVlZXC7r6/P+Hw+U11dPaQ+XNc1kigqZpWMEr0mFDUc5bruoNdCTD/jam1tld/vV0lJSXCf1+tVUVGRGhoawp7T29urw4cPh5yTlpamkpKSiOf09PQoEAiEFABgZIhpcPn9fklSdnZ2yP7s7Ozgsf/2ySefqK+vL6pzqqur5fV6g5WXlxeD7gEANrDyrsJ169bJdd1gnTx5MtEtAQCGSUyDKycnR5LU0dERsr+joyN47L+NHz9eo0aNiuocj8ejzMzMkAIAjAwxDa6pU6cqJydH9fX1wX2BQEDvvvuuiouLw56Tnp6uefPmhZzT39+v+vr6iOcAAEawaO9s6uzsNEeOHDFHjhwxkswzzzxjjhw5Ytra2owxxjz55JMmKyvLbNu2zbz//vumvLzcTJ061Xz22WfBOW6//XazcePG4PYrr7xiPB6Pqa2tNR9++KF56KGHTFZWlvH7/UPqibsKqVhXMkr0mlDUcNRQ7iqM+grdtWtX2B9WUVFhjPn3LfFPPPGEyc7ONh6PxyxYsMA0NzeHzJGfn2+qqqpC9m3cuNFMnjzZpKenm8LCQnPw4MEh90RwUbGuZJToNaGo4aihBJfz+QVhtUAgIK/Xm+g2kEKS8bLg93FhJHBdd9D7Fqy8qxAAMHIRXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAq4xOdANAMnIcJybzGGNiMg+A/+AVFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqBBcAwCoEFwDAKgQXAMAqUQfX3r17tXjxYvl8PjmOo7q6uuCxS5cuae3atZozZ46uueYa+Xw+3XfffTp9+vSAc65fv16O44TUrFmzon4wAIDUF3VwdXd3q6CgQDU1NVccu3DhghobG/XEE0+osbFRf/3rX9Xc3Kw777xz0Hlnz56tM2fOBGvfvn3RtgYAGAGi/kWSZWVlKisrC3vM6/Vqx44dIfueffZZFRYWqr29XZMnT47cyOjRysnJibYdAMAIE/fPuFzXleM4ysrKGnDc8ePH5fP5NG3aNC1fvlzt7e0Rx/b09CgQCIQUEEvGmJgUgNiLa3BdvHhRa9eu1bJly5SZmRlxXFFRkWpra7V9+3Zt2rRJra2tuvXWW9XZ2Rl2fHV1tbxeb7Dy8vLi9RAAAMnGXAVJZuvWrWGP9fb2msWLF5ubb77ZuK4b1byffvqpyczMNH/84x/DHr948aJxXTdYJ0+eNJIoKmaVjBK9JhQ1HDWUvIj6M66huHTpku6++261tbXp7bffHvDVVjhZWVmaMWOGWlpawh73eDzyeDyxaBUAYJmYv1V4ObSOHz+unTt36itf+UrUc3R1denEiRPKzc2NdXsAAMtFHVxdXV1qampSU1OTJKm1tVVNTU1qb2/XpUuX9OMf/1j//Oc/9ec//1l9fX3y+/3y+/3q7e0NzrFgwQI9++yzwe1HH31Ue/bs0ccff6wDBw7oBz/4gUaNGqVly5Zd/SMEAKSWaN9n37VrV9j3JSsqKkxra2vE9y137doVnCM/P99UVVUFt5cuXWpyc3NNenq6ue6668zSpUtNS0vLkHtyXTfh78tSqVXJKNFrQlHDUUP5jMv5/IKwWiAQkNfrTXQbSCHJeFk4jpPoFoC4c1130Psi+K5CAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFUILgCAVQguAIBVCC4AgFVGJ7oBIFaMMYlu4QqO4yS6BSDl8IoLAGAVggsAYBWCCwBgFYILAGAVggsAYJWog2vv3r1avHixfD6fHMdRXV1dyPEVK1bIcZyQWrRo0aDz1tTUaMqUKcrIyFBRUZEOHToUbWsAgBEg6uDq7u5WQUGBampqIo5ZtGiRzpw5E6yXX355wDlfffVVrVmzRlVVVWpsbFRBQYFKS0t19uzZaNsDAKQ6cxUkma1bt4bsq6ioMOXl5VHNU1hYaCorK4PbfX19xufzmerq6iGd77qukUSN8EpGiV4TirKtXNcd9LqKy2dcu3fv1sSJEzVz5kytWrVK586dizi2t7dXhw8fVklJSXBfWlqaSkpK1NDQEPacnp4eBQKBkAIAjAwxD65FixbpT3/6k+rr6/W73/1Oe/bsUVlZmfr6+sKO/+STT9TX16fs7OyQ/dnZ2fL7/WHPqa6ultfrDVZeXl6sHwYAIEnF/Cuf7rnnnuB/z5kzR3PnztX06dO1e/duLViwICY/Y926dVqzZk1wOxAIEF4AMELE/Xb4adOmafz48WppaQl7fPz48Ro1apQ6OjpC9nd0dCgnJyfsOR6PR5mZmSEFABgZ4h5cp06d0rlz55Sbmxv2eHp6uubNm6f6+vrgvv7+ftXX16u4uDje7QEALBN1cHV1dampqUlNTU2SpNbWVjU1Nam9vV1dXV36xS9+oYMHD+rjjz9WfX29ysvL9dWvflWlpaXBORYsWKBnn302uL1mzRr94Q9/0IsvvqiPPvpIq1atUnd3t376059e/SMEAKSWaG/v3bVrV9hbGCsqKsyFCxfMwoULzYQJE8yYMWNMfn6+WblypfH7/SFz5Ofnm6qqqpB9GzduNJMnTzbp6emmsLDQHDx4cMg9cTs8JXE7PEWlQg3ldnjn84vLaoFAQF6vN9FtIMGS8anM7+MCouO67qD3LfBdhQAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrEFwAAKsQXAAAqxBcAACrRB1ce/fu1eLFi+Xz+eQ4jurq6kKOO44TtjZs2BBxzvXr118xftasWVE/GABA6os6uLq7u1VQUKCampqwx8+cORNSmzdvluM4+tGPfjTgvLNnzw45b9++fdG2BgAYAUZHe0JZWZnKysoiHs/JyQnZ3rZtm77zne9o2rRpAzcyevQV5wIA8N/i+hlXR0eH3nzzTT3wwAODjj1+/Lh8Pp+mTZum5cuXq729PeLYnp4eBQKBkAIAjAxxDa4XX3xR48aN0w9/+MMBxxUVFam2tlbbt2/Xpk2b1NraqltvvVWdnZ1hx1dXV8vr9QYrLy8vHu1jBIv0WW20BSAOzFWQZLZu3Rrx+MyZM83q1aujnvfTTz81mZmZ5o9//GPY4xcvXjSu6wbr5MmTRhI1wiuWEv1YKGqkluu6g16fUX/GNVTvvPOOmpub9eqrr0Z9blZWlmbMmKGWlpawxz0ejzwez9W2CACwUNzeKnzhhRc0b948FRQURH1uV1eXTpw4odzc3Dh0BgCwWdTB1dXVpaamJjU1NUmSWltb1dTUFHIzRSAQ0Ouvv64HH3ww7BwLFizQs88+G9x+9NFHtWfPHn388cc6cOCAfvCDH2jUqFFatmxZtO0BAFJdtO/979q1K+z7khUVFcExzz//vBk7dqw5f/582Dny8/NNVVVVcHvp0qUmNzfXpKenm+uuu84sXbrUtLS0DLkn13UT/r4slfiKpUQ/FooaqTWUz7iczy9SqwUCAXm93kS3gQSL5VOZOwKBxHBdV5mZmQOO4bsKAQBWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFYhuAAAViG4AABWIbgAAFaJ2+/jAoYb3y8IjAy84gIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYJSWCyxiT6BYAADEwlD/PUyK4Ojs7E90CACAGhvLnuWNS4OVKf3+/Tp8+rXHjxslxnIjjAoGA8vLydPLkSWVmZg5jh1eHvoeXrX1L9vZO38MrGfs2xqizs1M+n09paQO/pho9TD3FVVpamiZNmjTk8ZmZmUnzPysa9D28bO1bsrd3+h5eyda31+sd0riUeKsQADByEFwAAKuMqODyeDyqqqqSx+NJdCtRoe/hZWvfkr290/fwsrXvy1Li5gwAwMgxol5xAQDsR3ABAKxCcAEArEJwAQCsknLBVVNToylTpigjI0NFRUU6dOjQgONff/11zZo1SxkZGZozZ47eeuutYer036qrq/WNb3xD48aN08SJE7VkyRI1NzcPeE5tba0cxwmpjIyMYer439avX39FD7NmzRrwnESvtSRNmTLlir4dx1FlZWXY8Ylc671792rx4sXy+XxyHEd1dXUhx40x+tWvfqXc3FyNHTtWJSUlOn78+KDzRnuNxLLvS5cuae3atZozZ46uueYa+Xw+3XfffTp9+vSAc/4vz7dY9i1JK1asuKKHRYsWDTpvItdbUtjnu+M42rBhQ8Q5h2O9r0ZKBderr76qNWvWqKqqSo2NjSooKFBpaanOnj0bdvyBAwe0bNkyPfDAAzpy5IiWLFmiJUuW6IMPPhi2nvfs2aPKykodPHhQO3bs0KVLl7Rw4UJ1d3cPeF5mZqbOnDkTrLa2tmHq+D9mz54d0sO+ffsijk2GtZak9957L6TnHTt2SJLuuuuuiOckaq27u7tVUFCgmpqasMefeuop/f73v9dzzz2nd999V9dcc41KS0t18eLFiHNGe43Euu8LFy6osbFRTzzxhBobG/XXv/5Vzc3NuvPOOwedN5rnW6z7vmzRokUhPbz88ssDzpno9ZYU0u+ZM2e0efNmOY6jH/3oRwPOG+/1viomhRQWFprKysrgdl9fn/H5fKa6ujrs+LvvvtvccccdIfuKiorMww8/HNc+B3L27FkjyezZsyfimC1bthiv1zt8TYVRVVVlCgoKhjw+GdfaGGN+9rOfmenTp5v+/v6wx5NhrY0xRpLZunVrcLu/v9/k5OSYDRs2BPedP3/eeDwe8/LLL0ecJ9prJNZ9h3Po0CEjybS1tUUcE+3z7WqF67uiosKUl5dHNU8yrnd5ebm5/fbbBxwz3OsdrZR5xdXb26vDhw+rpKQkuC8tLU0lJSVqaGgIe05DQ0PIeEkqLS2NOH44uK4rSbr22msHHNfV1aX8/Hzl5eWpvLxcx44dG472Qhw/flw+n0/Tpk3T8uXL1d7eHnFsMq51b2+vXnrpJd1///0DfjlzMqz1f2ttbZXf7w9ZU6/Xq6Kioohr+r9cI8PBdV05jqOsrKwBx0XzfIuX3bt3a+LEiZo5c6ZWrVqlc+fORRybjOvd0dGhN998Uw888MCgY5NhvSNJmeD65JNP1NfXp+zs7JD92dnZ8vv9Yc/x+/1RjY+3/v5+PfLII/rmN7+pm266KeK4mTNnavPmzdq2bZteeukl9ff3a/78+Tp16tSw9VpUVKTa2lpt375dmzZtUmtrq2699daIv5Ig2dZakurq6nT+/HmtWLEi4phkWOtwLq9bNGv6v1wj8Xbx4kWtXbtWy5YtG/DLXqN9vsXDokWL9Kc//Un19fX63e9+pz179qisrEx9fX1hxyfjer/44osaN26cfvjDHw44LhnWeyAp8e3wqaKyslIffPDBoO8lFxcXq7i4OLg9f/583XDDDXr++ef129/+Nt5tSpLKysqC/z137lwVFRUpPz9fr7322pD+NpcMXnjhBZWVlcnn80UckwxrnaouXbqku+++W8YYbdq0acCxyfB8u+eee4L/PWfOHM2dO1fTp0/X7t27tWDBgmHp4Wpt3rxZy5cvH/QGo2RY74GkzCuu8ePHa9SoUero6AjZ39HRoZycnLDn5OTkRDU+nlavXq033nhDu3btiupXtEjSmDFjdPPNN6ulpSVO3Q0uKytLM2bMiNhDMq21JLW1tWnnzp168MEHozovGdZaUnDdolnT/+UaiZfLodXW1qYdO3ZE/as1Bnu+DYdp06Zp/PjxEXtIpvWWpHfeeUfNzc1RP+el5FjvL0qZ4EpPT9e8efNUX18f3Nff36/6+vqQvzF/UXFxcch4SdqxY0fE8fFgjNHq1au1detWvf3225o6dWrUc/T19eno0aPKzc2NQ4dD09XVpRMnTkTsIRnW+ou2bNmiiRMn6o477ojqvGRYa0maOnWqcnJyQtY0EAjo3Xffjbim/8s1Eg+XQ+v48ePauXOnvvKVr0Q9x2DPt+Fw6tQpnTt3LmIPybLel73wwguaN2+eCgoKoj43GdY7RKLvDomlV155xXg8HlNbW2s+/PBD89BDD5msrCzj9/uNMcbce++95rHHHguO379/vxk9erR5+umnzUcffWSqqqrMmDFjzNGjR4et51WrVhmv12t2795tzpw5E6wLFy4Ex/x337/+9a/N3//+d3PixAlz+PBhc88995iMjAxz7NixYev75z//udm9e7dpbW01+/fvNyUlJWb8+PHm7NmzYXtOhrW+rK+vz0yePNmsXbv2imPJtNadnZ3myJEj5siRI0aSeeaZZ8yRI0eCd989+eSTJisry2zbts28//77pry83EydOtV89tlnwTluv/12s3HjxuD2YNdIvPvu7e01d955p5k0aZJpamoKec739PRE7Huw51u8++7s7DSPPvqoaWhoMK2trWbnzp3m61//urn++uvNxYsXI/ad6PW+zHVd86Uvfcls2rQp7ByJWO+rkVLBZYwxGzduNJMnTzbp6emmsLDQHDx4MHjstttuMxUVFSHjX3vtNTNjxgyTnp5uZs+ebd58881h7VdS2NqyZUvEvh955JHgY8zOzjbf+973TGNj47D2vXTpUpObm2vS09PNddddZ5YuXWpaWloi9mxM4tf6sr///e9Gkmlubr7iWDKt9a5du8I+Ny7319/fb5544gmTnZ1tPB6PWbBgwRWPKT8/31RVVYXsG+gaiXffra2tEZ/zu3btitj3YM+3ePd94cIFs3DhQjNhwgQzZswYk5+fb1auXHlFACXbel/2/PPPm7Fjx5rz58+HnSMR6301+LUmAACrpMxnXACAkYHgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFiF4AIAWIXgAgBYheACAFjl/wHU3xHeGgFBvAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing a single image\n",
    "dataset_size = len(train_dataset)\n",
    "random_index = random.randint(0, dataset_size - 1)\n",
    "\n",
    "if remove_border and not downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(20, 20)\n",
    "elif not remove_border and not upscaled_images and not downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(28, 28)\n",
    "elif downscaled_images:\n",
    "    image = train_loader.dataset[random_index][0].reshape(16, 16)\n",
    "else:\n",
    "    image = train_loader.dataset[random_index][0].reshape(32, 32)\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3973f0e3-5f32-4793-afc6-9394425419de",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e63be-c489-4aaf-a605-9e1d0fda631f",
   "metadata": {},
   "source": [
    "Converts csv into yaml config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9b89fd2-f200-40c1-973a-489ef4190e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define first input and the name of the file to be saved\n",
    "first_in_dim = 400 # 20x20\n",
    "filename = \"config/mnist_config_20x20.yaml\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dc64546-f1d7-4e18-b88c-7140d4d42692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file 'config/mnist_config_20x20.yaml' generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# reads the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"config/mnist_hyperparameters.csv\")\n",
    "\n",
    "# convert the DataFrame to a list of dictionaries\n",
    "models = df.to_dict(orient=\"records\")\n",
    "\n",
    "# create the YAML structure\n",
    "yaml_structure = {\"models\": {}}\n",
    "\n",
    "# rounds the number to the nearest multiple of the output size\n",
    "def round_to_nearest_multiple(value, multiple):\n",
    "    return multiple * round(value / multiple)\n",
    "\n",
    "# populate the YAML structure with models\n",
    "for i, model in enumerate(models, start=1):\n",
    "    # zero-padding model names to 3 digits \n",
    "    model_name = f\"model_{str(i).zfill(3)}\"\n",
    "    layers_config = {}\n",
    "    \n",
    "    for layer in range(1, model[\"H\"] + 1):\n",
    "        # zero-padding the layer names to 3 digits\n",
    "        layer_name = f\"LogicLayer{str(layer).zfill(3)}\"\n",
    "        \n",
    "        # adjusts in_dim to the nearest multiple of 10\n",
    "        in_dim = first_in_dim if layer == 1 else round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        # adjusts out_dim to the nearest multiple of 10\n",
    "        out_dim = round_to_nearest_multiple(model[\"W\"], 10)\n",
    "        \n",
    "        layers_config[layer_name] = {\n",
    "            \"in_dim\": in_dim,\n",
    "            \"out_dim\": out_dim,\n",
    "            \"device\": \"cuda\",\n",
    "            \"implementation\": \"cuda\",\n",
    "            \"connections\": \"random\",\n",
    "            \"grad_factor\": 2, # we can try different grad_factor values as well\n",
    "        }\n",
    "    \n",
    "    yaml_structure[\"models\"][model_name] = {\n",
    "        \"input_dim\": first_in_dim, \n",
    "        \"output_size\": 10, # for MNIST classification\n",
    "        \"tau\": model[\"tau\"],\n",
    "        \"learning_rate\": model[\"lr\"],\n",
    "        \"layers_config\": layers_config,\n",
    "    }\n",
    "\n",
    "# saves to a YAML file\n",
    "with open(f'{filename}', \"w\") as file:\n",
    "    yaml.dump(yaml_structure, file, default_flow_style=False)\n",
    "\n",
    "print(f\"YAML file '{filename}' generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e939ace-e4c3-4781-bdea-f9722ddb925e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Model Definition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4c832-0caf-4efc-9b71-8c2cf1faffa7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Model Function Declarations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb42a7a7-1a8b-4d5f-b2c6-6f68678e328b",
   "metadata": {},
   "source": [
    "Custom GroupSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1ead0c5-4377-4447-937c-353d12632fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomGroupSum(GroupSum):\n",
    "    \"\"\"\n",
    "    The CustomGroupSum module that extends GroupSum to include printing of class sums.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        # Use the original functionality from GroupSum\n",
    "        output = super().forward(x)\n",
    "\n",
    "        # Calculate the class sums for printing\n",
    "        if isinstance(x, PackBitsTensor):\n",
    "            class_sums = x.group_sum(self.k)\n",
    "        else:\n",
    "            class_sums = x.reshape(*x.shape[:-1], self.k, x.shape[-1] // self.k).sum(-1)\n",
    "\n",
    "        # Print the class sums before returning the output\n",
    "        print(\"Class Sums:\", class_sums)\n",
    "\n",
    "        # Return the processed output, as in the original GroupSum\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e473370c-ec74-455b-854c-f597d04144f3",
   "metadata": {},
   "source": [
    "DiffLogic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b244edec-9bd5-474d-9aaa-4f9aafe31b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffLogic(nn.Module):\n",
    "    def __init__(self, layers_config, output_size, tau=30):\n",
    "        \"\"\"\n",
    "        Initializes the DiffLogic model with the specified layer configurations, output size, and temperature parameter.\n",
    "\n",
    "        Args:\n",
    "            layers_config (dict): Configuration for each logic layer, including dimensions, device, implementation, connections, and grad factor.\n",
    "            output_size (int): The number of output groups (classes in a classification problem).\n",
    "            tau (int): Temperature parameter for the GroupSum operation.\n",
    "        \"\"\"\n",
    "        super(DiffLogic, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # stores the logic layers\n",
    "        layers = []\n",
    "        for layer_name, config in layers_config.items():\n",
    "            layer = LogicLayer(\n",
    "                in_dim=config['in_dim'],\n",
    "                out_dim=config['out_dim'],\n",
    "                device=config['device'],\n",
    "                implementation=config['implementation'],\n",
    "                connections=config['connections'],\n",
    "                grad_factor=config['grad_factor']       \n",
    "            )\n",
    "            layers.append(layer)\n",
    "            print(layer)\n",
    "        \n",
    "        self.logic_layers = nn.Sequential(*layers)\n",
    "        self.group = GroupSum(k=output_size, tau=tau)\n",
    "        #self.group = CustomGroupSum(k=output_size, tau=tau) \n",
    "        self.log_text = \"\"  # initializes logging string\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DiffLogic model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after processing through the logic layers and grouping operation.\n",
    "        \"\"\"\n",
    "        # moves tensor to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to('cuda')          \n",
    "        x = self.flatten(x)\n",
    "        logits = self.logic_layers(x)\n",
    "        group = self.group(logits)\n",
    "        return group\n",
    "    \n",
    "    def save(self, file_path, model_name='model'):\n",
    "        \"\"\"\n",
    "        Saves the model's state dictionary to the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path where the model will be saved.\n",
    "            model_name (str): Name of the saved model\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'connections': [layer.indices for layer in self.logic_layers if isinstance(layer, LogicLayer)]\n",
    "        }, os.path.join(file_path, f\"{model_name}.pth\"))\n",
    "        self.log_text += f\"Model saved to: {file_path}\\n\"\n",
    "\n",
    "    def load(self, file_path):\n",
    "        \"\"\"\n",
    "        Loads the model's state dictionary from the specified file path.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path from which the model will be loaded.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(file_path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "        # assigns connections to each LogicLayer\n",
    "        for idx, layer in enumerate(self.logic_layers):\n",
    "            if isinstance(layer, LogicLayer):\n",
    "                layer.indices = checkpoint['connections'][idx]\n",
    "\n",
    "        self.eval()\n",
    "        self.log_text += f\"Model loaded from: {file_path}\\n\"\n",
    "        \n",
    "    def get_accuracy(self, data_loader):\n",
    "        \"\"\"\n",
    "        Calculates the accuracy of the model against a data loader\n",
    "\n",
    "        Args:\n",
    "            data_loader: a DataLoader object, e.g. train_loader or test_loader\n",
    "\n",
    "        Returns:\n",
    "            float: The accuracy\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # ensures that model is in evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            for batch_inputs, batch_outputs in tqdm(data_loader, desc=\"Running Inference\"):\n",
    "                batch_inputs, batch_outputs = batch_inputs.to('cuda'), batch_outputs.to('cuda')\n",
    "\n",
    "                # forward pass to get predictions\n",
    "                outputs = self(batch_inputs)\n",
    "\n",
    "                # gets the predicted class (index of the maximum logit)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # counting correct predictions\n",
    "                total += batch_outputs.size(0)  # total number of samples in the batch\n",
    "                correct += (predicted == batch_outputs).sum().item()  # counting correct predictions\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def get_log(self):\n",
    "        \"\"\"\n",
    "        Retrieves the log text and clears the log after retrieval.\n",
    "\n",
    "        Returns:\n",
    "            str: The log text.\n",
    "        \"\"\"\n",
    "        log_copy = self.log_text\n",
    "        self.log_text = \"\"  # Clear the log after returning\n",
    "        return log_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5632fa27-5e05-41b0-b3a3-ac941714657b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        \"\"\"\n",
    "        Initializes the EarlyStopper to stop training if the performance doesn't improve after a certain number of epochs.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait for an improvement.\n",
    "            min_delta (float): Minimum change to consider an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def should_stop(self, current_loss):\n",
    "        \"\"\"\n",
    "        Check if training should stop based on the current loss.\n",
    "\n",
    "        Args:\n",
    "            current_loss (float): The current loss.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if training should stop, False otherwise.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = current_loss\n",
    "            return False\n",
    "        elif current_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = current_loss\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            print(\"EarlyStopper Triggered: \", self.counter)\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd97e0b-c6c4-4262-97ab-32136126ba39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea90015-6e45-49af-bb11-539c8811adcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize Hydra with the config path and job name\n",
    "with initialize(version_base=None, config_path=\"config\", job_name=\"ExpLogic\"):\n",
    "    cfg = compose(config_name=\"mnist_config_20x20\")\n",
    "\n",
    "# training loop for all models\n",
    "all_models_dict = {}\n",
    "num_epochs = 5\n",
    "file_path = 'trained_models/mnist_trained_20x20' # where to save your trained models\n",
    "\n",
    "# loops through all model configs and trains each of them\n",
    "for model_name, model_cfg in cfg.models.items():\n",
    "    print(f'training model {model_name}')\n",
    "\n",
    "    # tracking dictionary\n",
    "    all_models_dict[model_name] = {\n",
    "        'losses': [],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # initializes DiffLogic model and moves to CUDA if available\n",
    "        model = DiffLogic(layers_config=model_cfg['layers_config'], \n",
    "                          output_size=model_cfg['output_size'], \n",
    "                          tau=model_cfg['tau']).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # optimizer and loss criterion\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=model_cfg['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # early stopping\n",
    "        early_stopper = EarlyStopper(patience=5)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "            epoch_loss = 0  # to track loss for an epoch\n",
    "            \n",
    "            for batch_inputs, batch_outputs in loop:\n",
    "                # move data to the appropriate device\n",
    "                device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "                batch_inputs, batch_outputs = batch_inputs.to(device).double(), batch_outputs.to(device).long()\n",
    "\n",
    "                # forward pass through the model\n",
    "                predictions = model(batch_inputs)\n",
    "                loss = criterion(predictions, batch_outputs)\n",
    "\n",
    "                # zero gradients, backpropagates, and updates model parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # accumulating the loss for the epoch\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # caclulating the average loss for the epoch\n",
    "            epoch_loss /= len(train_loader)\n",
    "            all_models_dict[model_name]['losses'].append(epoch_loss)\n",
    "            print(f'Epoch {epoch+1} Loss: {epoch_loss}')\n",
    "\n",
    "            # checks for early stopping\n",
    "            if early_stopper.should_stop(epoch_loss):\n",
    "                print(f\"Early stopping triggered for {model_name} at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "        # saving trained model's state\n",
    "        model.save(file_path, model_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR TRAINING {model_name.upper()}: {str(e)}\")\n",
    "\n",
    "print(\"All models processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28a834-adac-4241-83a4-d9a23ac17db5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Model Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33048f62-fc21-4f87-b725-6eb470a66700",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicLayer(400, 2500, train)\n",
      "LogicLayer(2500, 2500, train)\n",
      "Evaluating model_077_weights.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 34/34 [00:01<00:00, 20.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model_077_weights.pth: 92.15%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# testing loop to test inferences\n",
    "trained_models_dir = 'trained_models/'\n",
    "\n",
    "# retrieves a list of all model files in the directory\n",
    "model_files = sorted([f for f in os.listdir(trained_models_dir) if f.endswith('.pth')])\n",
    "\n",
    "with initialize(version_base=None, config_path=\"config\", job_name=\"ExpLogic_Test\"):\n",
    "    cfg = compose(config_name=\"mnist_config_20x20\")\n",
    "\n",
    "# dictionary to store the trained models\n",
    "trained_models = {}\n",
    "trained_models_accuracies = {}\n",
    "\n",
    "# loops through all model files and calculates their accuracies\n",
    "for i, model_file in enumerate(model_files):\n",
    "    if model_file.endswith('_weights.pth'):\n",
    "        model_name = model_file.removesuffix('_weights.pth')\n",
    "    else:\n",
    "        model_name = model_file.removesuffix('.pth')\n",
    "    \n",
    "    model_cfg = cfg['models'][model_name]\n",
    "    \n",
    "    # instantiates the model and load its weights\n",
    "    model = DiffLogic(layers_config=model_cfg['layers_config'], \n",
    "                          output_size=model_cfg['output_size'], \n",
    "                          tau=model_cfg['tau']).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model_path = os.path.join(trained_models_dir, model_file)\n",
    "    print(f\"Evaluating {model_file}...\")\n",
    "\n",
    "    # loads the respective model\n",
    "    model.load(model_path)\n",
    "    \n",
    "    # calculates accuracy\n",
    "    accuracy = model.get_accuracy(test_loader)\n",
    "    \n",
    "    print(f\"Accuracy of {model_file}: {accuracy * 100:.2f}%\\n\")\n",
    "    \n",
    "    trained_models[i] = model\n",
    "    trained_models_accuracies[i] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc15d31-2d9b-42f4-8d07-039e9c972871",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Switching Probability**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decdce79-f993-4a5c-81d1-f0e5b64be619",
   "metadata": {},
   "source": [
    "Converting DiffLogic to LogicGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f72fa04-489d-4d62-98c0-fd2b9323c677",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LogicGraph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MNIST images: 100%|██████████| 211/211 [00:20<00:00, 10.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LogicGraph import * \n",
    "    \n",
    "print(\"Creating LogicGraph\")\n",
    "gall = LogicGraph(model)\n",
    "\n",
    "gall.compute_sf(train_loader)\n",
    "gall.compute_sp(np.ones((20,20))-0.5)\n",
    "\n",
    "gall.edges()[('L1_N2029', 'L2_N2124')][\"ab\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd40df1-6d15-4988-b510-6440f6ceffd0",
   "metadata": {},
   "source": [
    "#### **Difference of SP for Individual Classes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70e7db-e9f7-400d-998f-79e7773da32d",
   "metadata": {},
   "source": [
    "Find the TP, TN, FP, FN instances, and use this to create sub-datasets for specific cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81293fce-d505-43d5-aa21-0011f72df319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/all_images_train.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(all_predictions_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./datasets/all_predictions_test.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m---> 51\u001b[0m     all_images_train      \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./datasets/all_images_train.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     all_labels_train      \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./datasets/all_labels_train.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m     all_predictions_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./datasets/all_predictions_train.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/blue/woodard/mkunzlermaldaner/project/ExpLogic/explogic_env/lib/python3.9/site-packages/torch/serialization.py:594\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    592\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    598\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/blue/woodard/mkunzlermaldaner/project/ExpLogic/explogic_env/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/blue/woodard/mkunzlermaldaner/project/ExpLogic/explogic_env/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/all_images_train.pth'"
     ]
    }
   ],
   "source": [
    "saved = True \n",
    "\n",
    "def predict_and_categorize(model, data_loader, binarize=False, threshold=0.5):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.diff_logic_model.eval()\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        for batch_inputs, batch_outputs in tqdm(data_loader, desc=\"Predicting\"):\n",
    "            batch_inputs, batch_outputs = batch_inputs.to('cuda'), batch_outputs.to('cuda')\n",
    "\n",
    "            if binarize:\n",
    "                batch_inputs = (batch_inputs > threshold).float()\n",
    "\n",
    "            # Forward pass to get predictions\n",
    "            outputs = model.diff_logic_model(batch_inputs)\n",
    "\n",
    "            # Get the predicted class (index of the maximum logit)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_images.append(batch_inputs.cpu())\n",
    "            all_labels.append(batch_outputs.cpu())\n",
    "            all_predictions.append(predicted.cpu())\n",
    "\n",
    "    all_images = torch.cat(all_images)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "    return all_images, all_labels, all_predictions\n",
    "\n",
    "if not saved: \n",
    "    # Usage:\n",
    "    binarize = True  # Set to True if you want to binarize the images\n",
    "    threshold = 0.5  # Set the threshold for binarization if binarize is True\n",
    "\n",
    "    all_images_train, all_labels_train, all_predictions_train = predict_and_categorize(model, train_loader, binarize=binarize, threshold=threshold)\n",
    "    all_images_test,  all_labels_test,  all_predictions_test  = predict_and_categorize(model,  test_loader, binarize=binarize, threshold=threshold) \n",
    "\n",
    "    torch.save(     all_images_train, './datasets/all_images_train.pth')\n",
    "    torch.save(     all_labels_train, './datasets/all_labels_train.pth')\n",
    "    torch.save(all_predictions_train, './datasets/all_predictions_train.pth')\n",
    "\n",
    "    torch.save(     all_images_test, './datasets/all_images_test.pth')\n",
    "    torch.save(     all_labels_test, './datasets/all_labels_test.pth')\n",
    "    torch.save(all_predictions_test, './datasets/all_predictions_test.pth')\n",
    "\n",
    "else: \n",
    "    all_images_train      = torch.load('./datasets/all_images_train.pth')\n",
    "    all_labels_train      = torch.load('./datasets/all_labels_train.pth')\n",
    "    all_predictions_train = torch.load('./datasets/all_predictions_train.pth')\n",
    "\n",
    "    all_images_test       = torch.load('./datasets/all_images_test.pth')\n",
    "    all_labels_test       = torch.load('./datasets/all_labels_test.pth')\n",
    "    all_predictions_test  = torch.load('./datasets/all_predictions_test.pth')\n",
    "\n",
    "if not saved: \n",
    "    # Fix random seeds for reproducibility\n",
    "    torch.manual_seed(42)            \n",
    "    torch.cuda.manual_seed(42)        \n",
    "    np.random.seed(42)\n",
    "    # If using CUDA:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Init parameters\n",
    "    batch_size = 256   # this can be tuned as well \n",
    "    binarize   = True  # Set this to True if you want to binarize the images\n",
    "    threshold  = 0.5   # Set the threshold for binarization\n",
    "\n",
    "    # Create class-specific data loaders for training and testing sets\n",
    "    train_class_loaders = subset.create_class_dataloaders(train_dataset, batch_size, binarize=binarize, threshold=threshold)\n",
    "    test_class_loaders  = subset.create_class_dataloaders(test_dataset,  batch_size, binarize=binarize, threshold=threshold)\n",
    "\n",
    "    print(\"Saving the Class Datasets\")\n",
    "    torch.save(train_class_loaders, './datasets/train_class_loaders.pth')\n",
    "    torch.save(test_class_loaders, './datasets/test_class_loaders.pth')\n",
    "\n",
    "    # Create TN/TP/FN/FP datasets\n",
    "    general_datasets        = subset.create_tn_tp_fn_fp_datasets(all_images_train, all_labels_train, all_predictions_train)\n",
    "    class_specific_datasets = subset.create_class_specific_datasets(all_images_train, all_labels_train, all_predictions_train)\n",
    "\n",
    "    print(\"Saving the General Datasets\")\n",
    "    torch.save(general_datasets, './datasets/general_datasets.pth')\n",
    "\n",
    "    # For the general TN, TP, FN, FP datasets:\n",
    "    t_dataset = DataLoader(general_datasets['T'],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    f_dataset = DataLoader(general_datasets['F'],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "\n",
    "    print(\"Saving the General Datasets\")\n",
    "    torch.save(t_dataset, './datasets/t_dataset.pth')\n",
    "    torch.save(f_dataset, './datasets/f_dataset.pth')\n",
    "\n",
    "    # For class-specific datasets (e.g., for class 5):\n",
    "    print(\"Saving the Class-Specific Datasets\")\n",
    "    for i in range(10): \n",
    "        class_specific_datasets['TP_class'][i] = DataLoader(class_specific_datasets['TP_class'][i],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "        class_specific_datasets['FP_class'][i] = DataLoader(class_specific_datasets['FP_class'][i],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "        class_specific_datasets['TN_class'][i] = DataLoader(class_specific_datasets['TN_class'][i],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "        class_specific_datasets['FN_class'][i] = DataLoader(class_specific_datasets['FN_class'][i],batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
    "\n",
    "    torch.save(class_specific_datasets, './datasets/class_specific_datasets.pth')    \n",
    "else: \n",
    "    t_dataset               = torch.load('./datasets/t_dataset.pth')\n",
    "    f_dataset               = torch.load('./datasets/f_dataset.pth')\n",
    "    class_specific_datasets = torch.load('./datasets/class_specific_datasets.pth')\n",
    "    train_class_loaders     = torch.load('./datasets/train_class_loaders.pth')\n",
    "    test_class_loaders      = torch.load('./datasets/test_class_loaders.pth')\n",
    "    general_datasets        = torch.load('./datasets/general_datasets.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283f3f3-ef17-4774-a590-02489ad2d667",
   "metadata": {},
   "source": [
    "Class Specific Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cca78da-be99-405f-86d3-2087cc506b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m sp_diffs \u001b[38;5;241m=\u001b[39m [] \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, output_node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs): \n\u001b[0;32m----> 7\u001b[0m     p_maps\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcv2\u001b[49m\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist_averages/average_digit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m24\u001b[39m,\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m24\u001b[39m,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m      8\u001b[0m     sp_graph  \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(gall)\n\u001b[1;32m      9\u001b[0m     sp_graph\u001b[38;5;241m.\u001b[39mcompute_sp(p_maps[j])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "outputs = [f\"L3_N0\", f\"L3_N1\", f\"L3_N2\", f\"L3_N3\", f\"L3_N4\", f\"L3_N5\", f\"L3_N6\", f\"L3_N7\", f\"L3_N8\", f\"L3_N9\"]\n",
    "\n",
    "p_maps = []\n",
    "sp_graphs = [] \n",
    "sp_diffs = [] \n",
    "for j, output_node in enumerate(outputs): \n",
    "    p_maps.append(cv2.imread(f\"mnist_averages/average_digit_{j}.png\")[4:24,4:24,1]/255)\n",
    "    sp_graph  = copy.deepcopy(gall)\n",
    "    sp_graph.compute_sp(p_maps[j])\n",
    "    sp_graphs.append(sp_graph)\n",
    "    sp_diffs.append(sp_graph.sub_sp(gall))\n",
    "    \n",
    "# Compute SF for each graph\n",
    "graphs = {}\n",
    "for g in range(10): \n",
    "    print(f\"Class {g}\")\n",
    "    graphs[g] = LogicGraph(model.diff_logic_model)\n",
    "    graphs[g].compute_sf(train_class_loaders[g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497f639-1ec5-44a7-a244-a1aa222bb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SF for each graph\n",
    "fps = {}\n",
    "tps = {}\n",
    "fns = {}\n",
    "tns = {}\n",
    "for g in range(10): \n",
    "    print(g)\n",
    "    fps[g] = LogicGraph(model.diff_logic_model)\n",
    "    if [i for i in class_specific_datasets['FP_class'][g]] != []:\n",
    "        fps[g].compute_sf(class_specific_datasets['FP_class'][g])\n",
    "    #tps[g] = LogicGraph(model.diff_logic_model)\n",
    "    #if [i for i in class_specific_datasets['TP_class'][g]] != []:\n",
    "    #    tps[g].compute_sf(class_specific_datasets['TP_class'][g])\n",
    "    fns[g] = LogicGraph(model.diff_logic_model)\n",
    "    if [i for i in class_specific_datasets['FN_class'][g]] != []:\n",
    "        fns[g].compute_sf(class_specific_datasets['FN_class'][g])\n",
    "    tns[g] = LogicGraph(model.diff_logic_model)\n",
    "    if [i for i in class_specific_datasets['TN_class'][g]] != []:\n",
    "        tns[g].compute_sf(class_specific_datasets['TN_class'][g])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ef565-a0b6-4dbf-a8b8-adcaf386825a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Hardware**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c06262-afe8-4b02-883c-c044a060fc39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Verilog Conversion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e20496-c3ac-4d87-8e55-359ac0069b03",
   "metadata": {},
   "source": [
    "Logic gate to Verilog expression mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846e8b8-9b6f-49dc-adbc-7bfe8fe0a27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logic_gate_verilog = {\n",
    "    \"0\": \"1'b0\",\n",
    "    \"A∧B\": \"({a}) & ({b})\",\n",
    "    \"¬(A⇒B)\": \"({a}) & ~({b})\",\n",
    "    \"A\": \"{a}\",\n",
    "    \"¬(B⇒A)\": \"({b}) & ~({a})\",\n",
    "    \"B\": \"{b}\",\n",
    "    \"A⊕B\": \"({a}) ^ ({b})\",\n",
    "    \"A∨B\": \"({a}) | ({b})\",\n",
    "    \"¬(A∨B)\": \"~(({a}) | ({b}))\",\n",
    "    \"¬(A⊕B)\": \"~(({a}) ^ ({b}))\",\n",
    "    \"¬B\": \"~({b})\",\n",
    "    \"B⇒A\": \"~({b}) | ({a})\",\n",
    "    \"¬A\": \"~({a})\",\n",
    "    \"A⇒B\": \"~({a}) | ({b})\",\n",
    "    \"¬(A∧B)\": \"~(({a}) & ({b}))\",\n",
    "    \"1\": \"1'b1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a28141-16b1-4e15-8adc-a4aa7cb405cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "side = 16 # pixels in one side of the image\n",
    "N_input = 16 * 16  # number of input dimensions\n",
    "\n",
    "# converts the learned logic gates to verilog or vhdl \n",
    "def generate_verilog(model, filename=\"logic_network.v\"):\n",
    "    \n",
    "    N_layers = len(model.logic_layers)\n",
    "\n",
    "    # gets number of neurons per layer\n",
    "    neurons_per_layer = [layer.weights.size()[0] for layer in model.logic_layers]\n",
    "    \n",
    "    # Set the output size to the number of neurons in the last layer\n",
    "    N_output = neurons_per_layer[-1]\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        # module declaration\n",
    "        file.write(\"module logic_network(\\n\")\n",
    "        file.write(f\"    input wire [{N_input-1}:0] inputs,\\n\")\n",
    "        file.write(f\"    output wire [{N_output-1}:0] outputs\\n\")\n",
    "        file.write(\");\\n\\n\")\n",
    "\n",
    "        # declares wires for internal layers\n",
    "        for layer_index in range(N_layers - 1):\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "            file.write(f\"    wire [{N_neurons -1}:0] layer{layer_index}_outputs;\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "        logic_operations = list(logic_gate_verilog.keys())\n",
    "\n",
    "        for layer_index in range(N_layers):\n",
    "            logic_layer = model.logic_layers[layer_index]\n",
    "\n",
    "            # gets input and output indices\n",
    "            input_indices = logic_layer.indices[0].cpu().numpy()  # first input indices\n",
    "            output_indices = logic_layer.indices[1].cpu().numpy()  # second input indices\n",
    "\n",
    "            neuron_gates = [torch.argmax(logic_layer.weights[neuron]).item()\n",
    "                            for neuron in range(logic_layer.weights.size()[0])]\n",
    "            connections = {i: (input_indices[i], output_indices[i]) for i in range(len(neuron_gates))}\n",
    "\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "\n",
    "            # determines input wires\n",
    "            if layer_index == 0:\n",
    "                input_wire_base = \"inputs\"\n",
    "            else:\n",
    "                input_wire_base = f\"layer{layer_index -1}_outputs\"\n",
    "\n",
    "            # determines output wires\n",
    "            if layer_index == N_layers - 1:\n",
    "                output_wire_base = \"outputs\"\n",
    "            else:\n",
    "                output_wire_base = f\"layer{layer_index}_outputs\"\n",
    "\n",
    "            # assign statements for this layer\n",
    "            for neuron_id in range(N_neurons):\n",
    "                a_idx, b_idx = connections[neuron_id]\n",
    "\n",
    "                # maps indices to input wires\n",
    "                a_wire = f\"{input_wire_base}[{a_idx}]\"\n",
    "                b_wire = f\"{input_wire_base}[{b_idx}]\"\n",
    "\n",
    "                # gets gate\n",
    "                gate_op = logic_operations[neuron_gates[neuron_id]]\n",
    "                gate = logic_gate_verilog[gate_op].format(a=a_wire, b=b_wire)\n",
    "\n",
    "                # assigns to output wire\n",
    "                output_wire = f\"{output_wire_base}[{neuron_id}]\"\n",
    "\n",
    "                file.write(f\"    assign {output_wire} = {gate};\\n\")\n",
    "\n",
    "        file.write(\"endmodule\\n\")\n",
    "        print('success')\n",
    "\n",
    "# generates Verilog file for all trained models\n",
    "for model_idx in range(len(trained_models)):\n",
    "    i = model_idx + 1\n",
    "    generate_verilog(trained_models[model_idx], filename=f\"verilog/{side}x{side}/model_{i:03d}_logic_network.v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7325be-1e5f-4c16-8a49-a66327d183bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **VHDL Conversion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22f5f2-059b-453e-b32e-9634cf8a9e3c",
   "metadata": {},
   "source": [
    "Logic gate to Verilog expression mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2f39a-e420-4b4d-93a6-1c0182bd15b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logic_gate_vhdl = {\n",
    "    \"0\": \"'0'\",\n",
    "    \"A∧B\": \"({a}) and ({b})\",\n",
    "    \"¬(A⇒B)\": \"({a}) and not ({b})\",\n",
    "    \"A\": \"{a}\",\n",
    "    \"¬(B⇒A)\": \"({b}) and not ({a})\",\n",
    "    \"B\": \"{b}\",\n",
    "    \"A⊕B\": \"({a}) xor ({b})\",\n",
    "    \"A∨B\": \"({a}) or ({b})\",\n",
    "    \"¬(A∨B)\": \"not(({a}) or ({b}))\",\n",
    "    \"¬(A⊕B)\": \"not(({a}) xor ({b}))\",\n",
    "    \"¬B\": \"not({b})\",\n",
    "    \"B⇒A\": \"not({b}) or ({a})\",\n",
    "    \"¬A\": \"not({a})\",\n",
    "    \"A⇒B\": \"not({a}) or ({b})\",\n",
    "    \"¬(A∧B)\": \"not(({a}) and ({b}))\",\n",
    "    \"1\": \"'1'\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20389672-5297-4dd4-a368-1b96df1445ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "side = 16 # pixels in one side of the image\n",
    "N_input = 16 * 16  # number of input dimensions\n",
    "\n",
    "# Converts the learned logic gates to VHDL\n",
    "def generate_vhdl(model, filename=\"logic_network.vhdl\"):\n",
    "    N_layers = len(model.logic_layers)\n",
    "    neurons_per_layer = [layer.weights.size()[0] for layer in model.logic_layers]\n",
    "\n",
    "    # set output size to the number of neurons in the last layer\n",
    "    N_output = neurons_per_layer[-1]\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        # Library and entity declaration\n",
    "        file.write(\"library IEEE;\\n\")\n",
    "        file.write(\"use IEEE.STD_LOGIC_1164.ALL;\\n\\n\")\n",
    "        file.write(\"entity logic_network is\\n\")\n",
    "        file.write(f\"    port (\\n\")\n",
    "        file.write(f\"        inputs : in std_logic_vector({N_input - 1} downto 0);\\n\")\n",
    "        file.write(f\"        outputs : out std_logic_vector({N_output - 1} downto 0)\\n\")\n",
    "        file.write(\"    );\\n\")\n",
    "        file.write(\"end logic_network;\\n\\n\")\n",
    "        file.write(\"architecture Behavioral of logic_network is\\n\")\n",
    "\n",
    "        # Declare signals for internal layers\n",
    "        for layer_index in range(N_layers - 1):\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "            file.write(f\"    signal layer{layer_index}_outputs : std_logic_vector({N_neurons - 1} downto 0);\\n\")\n",
    "        file.write(\"\\nbegin\\n\\n\")\n",
    "\n",
    "        logic_operations = list(logic_gate_vhdl.keys())\n",
    "\n",
    "        # Generate VHDL code for each layer\n",
    "        for layer_index in range(N_layers):\n",
    "            logic_layer = model.logic_layers[layer_index]\n",
    "\n",
    "            # Get input and output indices\n",
    "            input_indices = logic_layer.indices[0].cpu().numpy()  # first input indices\n",
    "            output_indices = logic_layer.indices[1].cpu().numpy()  # second input indices\n",
    "\n",
    "            neuron_gates = [torch.argmax(logic_layer.weights[neuron]).item()\n",
    "                            for neuron in range(logic_layer.weights.size()[0])]\n",
    "            connections = {i: (input_indices[i], output_indices[i]) for i in range(len(neuron_gates))}\n",
    "\n",
    "            N_neurons = neurons_per_layer[layer_index]\n",
    "\n",
    "            # Determine input signals\n",
    "            if layer_index == 0:\n",
    "                input_wire_base = \"inputs\"\n",
    "            else:\n",
    "                input_wire_base = f\"layer{layer_index -1}_outputs\"\n",
    "\n",
    "            # Determine output signals\n",
    "            if layer_index == N_layers - 1:\n",
    "                output_wire_base = \"outputs\"\n",
    "            else:\n",
    "                output_wire_base = f\"layer{layer_index}_outputs\"\n",
    "\n",
    "            # Assign statements for each neuron in this layer\n",
    "            for neuron_id in range(N_neurons):\n",
    "                a_idx, b_idx = connections[neuron_id]\n",
    "\n",
    "                # Map indices to input signals\n",
    "                a_wire = f\"{input_wire_base}({a_idx})\"\n",
    "                b_wire = f\"{input_wire_base}({b_idx})\"\n",
    "\n",
    "                # Get the gate operation\n",
    "                gate_op = logic_operations[neuron_gates[neuron_id]]\n",
    "                gate = logic_gate_vhdl[gate_op].format(a=a_wire, b=b_wire)\n",
    "\n",
    "                # Assign to output signal\n",
    "                output_wire = f\"{output_wire_base}({neuron_id})\"\n",
    "                file.write(f\"    {output_wire} <= {gate};\\n\")\n",
    "\n",
    "        file.write(\"\\nend Behavioral;\\n\")\n",
    "        print('success')\n",
    "\n",
    "# Generates VHDL files for all trained models\n",
    "for model_idx in range(len(trained_models)):\n",
    "    i = model_idx + 1\n",
    "    generate_vhdl(trained_models[model_idx], filename=f\"vhdl/{side}x{side}/model_{i:03d}_logic_network.vhdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d3597-9cf6-4688-a0ab-a676848f4439",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Predicting from Hex**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59df202-6740-4136-82ad-2ad266c20eb6",
   "metadata": {},
   "source": [
    "Sanity check to see that the FPGA output matches the model predictions on the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac4a4f5-4b7f-47be-bf77-6864016839c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def load_images_from_hex_file(filename):\n",
    "    images = []\n",
    "    hex_pattern = re.compile(r'^[0-9a-fA-F]+$')  # only allows valid hex characters\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if ':' in line:\n",
    "                hex_data = line.split(':')[1].strip().strip(';')\n",
    "                \n",
    "                # check if hex_data is a valid hexadecimal string\n",
    "                if hex_pattern.match(hex_data):\n",
    "                    # convert to binary and pad to 256 bits\n",
    "                    bin_data = bin(int(hex_data, 16))[2:].zfill(256)\n",
    "                    image = np.array([int(bit) for bit in bin_data], dtype=np.uint8).reshape(16, 16)\n",
    "                    images.append(image)\n",
    "                else:\n",
    "                    print(f\"Skipping invalid line: {line.strip()}\")\n",
    "\n",
    "    return np.array(images)\n",
    "\n",
    "# loading images\n",
    "filename = 'mnist_input.mif'  \n",
    "images = load_images_from_hex_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7da784-e8c6-47b5-af6f-4c8b09e1dcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference_on_images(model, images):\n",
    "    model.eval()  # sets model to evaluation mode\n",
    "    predictions = []\n",
    "    \n",
    "    # starts inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for img in images:\n",
    "        # flatten each 16x16 image to a 1D tensor with 256 elements\n",
    "        input_data = torch.tensor(img.flatten(), dtype=torch.float32).unsqueeze(0).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        with torch.no_grad():  # disable gradient computation for inference\n",
    "            output = model(input_data)\n",
    "            _, predicted_label = torch.max(output, 1)  # get the predicted label\n",
    "            predictions.append(predicted_label.item())\n",
    "    \n",
    "    # end timing inference\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time  # total inference time in seconds\n",
    "    avg_inference_time = inference_time / len(images)\n",
    "    \n",
    "    return predictions, avg_inference_time\n",
    "\n",
    "# dictionary to store the predictions and avg inference times for each of the 25 models    \n",
    "predictions = {}\n",
    "avg_inference_times = {}\n",
    "\n",
    "for model_idx in trained_models:\n",
    "    prediction, avg_inference_time = run_inference_on_images(trained_models[model_idx], images)\n",
    "    predictions[model_idx] = prediction\n",
    "    avg_inference_times[model_idx] = avg_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d6ebd-1029-41b0-945b-e255cca2ec6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from difflogic import CompiledLogicNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689d378-2324-496d-bcb2-6d06d5220c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference_with_packbits(model, images):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "\n",
    "    # Start timing the inference\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop through each image\n",
    "    with torch.no_grad():  # No gradient computation needed\n",
    "        for img in images:\n",
    "            # Convert image to boolean tensor and flatten\n",
    "            print(img)\n",
    "            img_tensor = torch.tensor(img, dtype=torch.bool).view(1, -1)  # Shape: [1, flattened_dim]\n",
    "            print(img_tensor)\n",
    "                \n",
    "            # Convert to PackBitsTensor for efficient inference\n",
    "            packed_input = difflogic.PackBitsTensor(img_tensor)\n",
    "            print(packed_input)\n",
    "            \n",
    "            # Perform inference with PackBitsTensor\n",
    "            output = model(packed_input)\n",
    "\n",
    "            # Get predicted class\n",
    "            _, predicted_label = torch.max(output, 1)\n",
    "            predictions.append(predicted_label.item())\n",
    "\n",
    "    # End timing the inference\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time  # Total inference time in seconds\n",
    "    avg_inference_time = inference_time / len(images)\n",
    "\n",
    "    return predictions, avg_inference_time\n",
    "\n",
    "# Assuming `images` is a numpy array of shape [num_images, height, width]\n",
    "predictions = {}\n",
    "avg_inference_times = {}\n",
    "\n",
    "model = trained_models\n",
    "model.implementation = 'cuda'\n",
    "for model_idx, trained_model in trained_models.items():\n",
    "    prediction, avg_inference_time = run_inference_with_packbits(trained_model, images)\n",
    "    predictions[model_idx] = prediction\n",
    "    avg_inference_times[model_idx] = avg_inference_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca015aa-a294-4600-a80b-d4c9e41438d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# displays some sample images with predictions\n",
    "def display_images_side_by_side(images, predictions, num_images=10, title=\"Model\", accuracy=None, avg_inference_time=None):\n",
    "    \n",
    "    # converts avg_inference_time from seconds to milliseconds\n",
    "    avg_inference_time_ms = avg_inference_time * 1000 if avg_inference_time is not None else 0\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 2))  \n",
    "    \n",
    "    # sets big title above all images with accuracy and inference time\n",
    "    fig.suptitle(f\"{title}\\nAccuracy: {accuracy:.2%} | Avg Inference Time: {avg_inference_time_ms:.2f} ms\", \n",
    "                 fontsize=14, fontweight='bold', y=1.1)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(images[i], cmap='gray')\n",
    "        axes[i].set_title(f\"Pred: {predictions[i]}\")\n",
    "        axes[i].axis('off')  \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# loops through each model and display images with respective accuracy and inference time\n",
    "for model_name, preds in predictions.items():\n",
    "    # gets accuracy and inference time for the current model\n",
    "    accuracy = trained_models_accuracies[model_name]\n",
    "    avg_inference_time = avg_inference_times[model_name]\n",
    "    # displaying the first 10 images side by side with their predictions\n",
    "    display_images_side_by_side(\n",
    "        images, preds, num_images=10, \n",
    "        title=f'Model {model_name}', \n",
    "        accuracy=accuracy, \n",
    "        avg_inference_time=avg_inference_time\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExpLogic",
   "language": "python",
   "name": "difflogic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
